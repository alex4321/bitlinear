[
  {
    "objectID": "03_mistral.html",
    "href": "03_mistral.html",
    "title": "bitlinear",
    "section": "",
    "text": "BitMistralMLP\n\n BitMistralMLP\n                (config:transformers.models.mistral.configuration_mistral.\n                MistralConfig, fname_prefix:str, base:Optional[transformer\n                s.models.mistral.modeling_mistral.MistralMLP]=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nBitMistralSdpaAttention\n\n BitMistralSdpaAttention\n                          (config:transformers.models.mistral.configuratio\n                          n_mistral.MistralConfig, fname_prefix:str,\n                          layer_idx:Optional[int]=None, base:Optional[tran\n                          sformers.models.mistral.modeling_mistral.Mistral\n                          SdpaAttention]=None)\n\nMistral attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from MistralAttention as the weights of the module stays untouched. The only changes are on the forward pass to adapt to SDPA API.\n\n\n\nBitMistralFlashAttention2\n\n BitMistralFlashAttention2\n                            (config:transformers.models.mistral.configurat\n                            ion_mistral.MistralConfig, fname_prefix:str,\n                            layer_idx:Optional[int]=None, base:Optional[tr\n                            ansformers.models.mistral.modeling_mistral.Mis\n                            tralFlashAttention2]=None)\n\nMistral flash attention module. This module inherits from MistralAttention as the weights of the module stays untouched. The only required change would be on the forward pass where it needs to correctly call the public API of flash attention and deal with padding tokens in case the input contains any of them.\n\n\n\nBitMistralAttention\n\n BitMistralAttention\n                      (config:transformers.models.mistral.configuration_mi\n                      stral.MistralConfig, fname_prefix:str,\n                      layer_idx:Optional[int]=None, base:Optional[transfor\n                      mers.models.mistral.modeling_mistral.MistralAttentio\n                      n]=None)\n\nMulti-headed attention from ‘Attention Is All You Need’ paper. Modified to use sliding window attention: Longformer and “Generating Long Sequences with Sparse Transformers”.\n\n\n\nBitMistralAttentionBase\n\n BitMistralAttentionBase\n                          (config:transformers.models.mistral.configuratio\n                          n_mistral.MistralConfig, fname_prefix:str,\n                          layer_idx:Optional[int]=None, base:Optional[tran\n                          sformers.models.mistral.modeling_mistral.Mistral\n                          Attention]=None)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nBitMistralDecoderLayer\n\n BitMistralDecoderLayer\n                         (config:transformers.models.mistral.configuration\n                         _mistral.MistralConfig, layer_idx:int,\n                         fname_prefix:str, base:Optional[transformers.mode\n                         ls.mistral.modeling_mistral.MistralDecoderLayer]=\n                         None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nBitMistralAdaptersMixin\n\n BitMistralAdaptersMixin (*args, **kwargs)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nBitMistralPreTrainedModel\n\n BitMistralPreTrainedModel\n                            (config:transformers.configuration_utils.Pretr\n                            ainedConfig, *inputs, **kwargs)\n\nThe bare Mistral Model outputting raw hidden-states without any specific head on top. This model inherits from [PreTrainedModel]. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\nThis model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\nParameters: config ([MistralConfig]): Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the [~PreTrainedModel.from_pretrained] method to load the model weights.\n\n\n\nBitMistralModel\n\n BitMistralModel\n                  (config:transformers.models.mistral.configuration_mistra\n                  l.MistralConfig, fname_prefix:str, base:Optional[transfo\n                  rmers.models.mistral.modeling_mistral.MistralModel]=None\n                  )\n\nThe bare Mistral Model outputting raw hidden-states without any specific head on top. This model inherits from [PreTrainedModel]. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\nThis model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\nParameters: config ([MistralConfig]): Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the [~PreTrainedModel.from_pretrained] method to load the model weights.\nTransformer decoder consisting of config.num_hidden_layers layers. Each layer is a [MistralDecoderLayer]\nArgs: config: MistralConfig\n\n\n\nBitMistralForCausalLM\n\n BitMistralForCausalLM\n                        (config:transformers.models.mistral.configuration_\n                        mistral.MistralConfig, fname_prefix:str, base:Opti\n                        onal[transformers.models.mistral.modeling_mistral.\n                        MistralForCausalLM]=None)\n\nThe bare Mistral Model outputting raw hidden-states without any specific head on top. This model inherits from [PreTrainedModel]. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\nThis model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\nParameters: config ([MistralConfig]): Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the [~PreTrainedModel.from_pretrained] method to load the model weights.\n\n\n\nBitMistralForSequenceClassification\n\n BitMistralForSequenceClassification\n                                      (config:transformers.models.mistral.\n                                      configuration_mistral.MistralConfig,\n                                      fname_prefix:str, base:Optional[tran\n                                      sformers.models.mistral.modeling_mis\n                                      tral.MistralForSequenceClassificatio\n                                      n]=None)\n\nThe Mistral Model transformer with a sequence classification head on top (linear layer).\n[MistralForSequenceClassification] uses the last token in order to do the classification, as other causal models (e.g. GPT-2) do.\nSince it does classification on the last token, it requires to know the position of the last token. If a pad_token_id is defined in the configuration, it finds the last token that is not a padding token in each row. If no pad_token_id is defined, it simply takes the last value in each row of the batch. Since it cannot guess the padding tokens when inputs_embeds are passed instead of input_ids, it does the same (take the last value in each row of the batch).\nThis model inherits from [PreTrainedModel]. Check the superclass documentation for the generic methods the library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads etc.)\nThis model is also a PyTorch torch.nn.Module subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and behavior.\nParameters: config ([MistralConfig]): Model configuration class with all the parameters of the model. Initializing with a config file does not load the weights associated with the model, only the configuration. Check out the [~PreTrainedModel.from_pretrained] method to load the model weights.",
    "crumbs": [
      "03_mistral.html"
    ]
  },
  {
    "objectID": "adapters.html",
    "href": "adapters.html",
    "title": "adapters",
    "section": "",
    "text": "Basically the idea of low-rank adapters is the following one:\n$ Linear(W, x) = W x $ where \\(W\\) is \\((outputFeatures, inputFeatures)\\) matrix and \\(x\\) is \\((inputFeatures, batchSize)\\) matrix\nSo if W_changed is \\(W + \\Delta W\\):\n$ Linear(W_changed, x) = W_changed x = (W + W) x = W x + W x $\nAnd if \\(\\Delta W\\) is a low-rank matrix than we can represent it as\n$ W = lora_B lora_A $\nWhere \\(lora_B\\) is \\((outputFeatures, loraRank)\\) matrix and \\(lora_A\\) is \\((loraRank, inputFeatures)\\) matrix\nSo\n$ Linear(W_changed, x) = W x + (lora_B lora_A) x $\nBut since \\(\\Delta W\\) matrix itself is relatively big and the matrix multiplication is an associative operation - we can make it the following way:\n$ Linear(W_changed, x) = W x + lora_B (lora_A x) $",
    "crumbs": [
      "adapters"
    ]
  },
  {
    "objectID": "adapters.html#idea",
    "href": "adapters.html#idea",
    "title": "adapters",
    "section": "",
    "text": "Basically the idea of low-rank adapters is the following one:\n$ Linear(W, x) = W x $ where \\(W\\) is \\((outputFeatures, inputFeatures)\\) matrix and \\(x\\) is \\((inputFeatures, batchSize)\\) matrix\nSo if W_changed is \\(W + \\Delta W\\):\n$ Linear(W_changed, x) = W_changed x = (W + W) x = W x + W x $\nAnd if \\(\\Delta W\\) is a low-rank matrix than we can represent it as\n$ W = lora_B lora_A $\nWhere \\(lora_B\\) is \\((outputFeatures, loraRank)\\) matrix and \\(lora_A\\) is \\((loraRank, inputFeatures)\\) matrix\nSo\n$ Linear(W_changed, x) = W x + (lora_B lora_A) x $\nBut since \\(\\Delta W\\) matrix itself is relatively big and the matrix multiplication is an associative operation - we can make it the following way:\n$ Linear(W_changed, x) = W x + lora_B (lora_A x) $",
    "crumbs": [
      "adapters"
    ]
  },
  {
    "objectID": "adapters.html#implementation",
    "href": "adapters.html#implementation",
    "title": "adapters",
    "section": "Implementation",
    "text": "Implementation\n\n\nLinearAdapter\n\n LinearAdapter (*args, **kwargs)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nLoRAAdapter\n\n LoRAAdapter (in_features:int, out_features:int, lora_rank:int,\n              device=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\n\n\nMergeableLayer\n\n MergeableLayer (adapter:__main__.LinearAdapter)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool",
    "crumbs": [
      "adapters"
    ]
  },
  {
    "objectID": "bitlinear.html",
    "href": "bitlinear.html",
    "title": "bitlinear",
    "section": "",
    "text": "This paper https://arxiv.org/pdf/2402.17764.pdf suggest that we can use a linear layer which weight can effectively only get three values \\((-scale, 0, scale)\\)\nBut for pretraining purpose they still use full (or half) precision weights, effectively do quantization only during the forward pass:\n$ ThreeValuesLinear(W, x) = ThreeValuesWeight(W) x $\n$ ThreeValuesWeight(W) = sign({ W - {(W) } }) { {|W|} } $\nSo \\(sign(...)\\) part effectively make it \\((-1, 0, 1)\\) values, and multiplying to a \\(scale\\) converts to \\((-scale, 0, scale)\\)\nBut since it’s not clear how to introduce gradient for \\(sign\\), as well as how to perform updated over quantized weights - they use non-quantized weights and perform quantization as a forward pass.\nBut there is a method - https://arxiv.org/pdf/2307.05695.pdf paper introduce a ReLoRA approach - making a high-rank updates via a sequence of low-rank ones.\nSo I am gonna try the following approach:\n\nInitialize \\(W\\) weights\nSave them to a temporary file \\(File_W\\)\nQuantize \\(W\\) weights to \\(W_quant\\)\nUnload \\(W\\) weights\nFor a few iterations\n\nTrain \\(i\\)-th LoRA adapter via ReLoRA procedure\nLoad \\(W\\) weights from \\(File_W\\)\nMerge with \\(\\Delta W\\) we got from the adapter: \\(W = W + \\Delta W\\)\nSave them to a temporary file \\(File_W\\)\nQuantize \\(W\\) weights to \\(W_quant\\)\nUnload \\(W\\) weights",
    "crumbs": [
      "bitlinear"
    ]
  },
  {
    "objectID": "bitlinear.html#idea",
    "href": "bitlinear.html#idea",
    "title": "bitlinear",
    "section": "",
    "text": "This paper https://arxiv.org/pdf/2402.17764.pdf suggest that we can use a linear layer which weight can effectively only get three values \\((-scale, 0, scale)\\)\nBut for pretraining purpose they still use full (or half) precision weights, effectively do quantization only during the forward pass:\n$ ThreeValuesLinear(W, x) = ThreeValuesWeight(W) x $\n$ ThreeValuesWeight(W) = sign({ W - {(W) } }) { {|W|} } $\nSo \\(sign(...)\\) part effectively make it \\((-1, 0, 1)\\) values, and multiplying to a \\(scale\\) converts to \\((-scale, 0, scale)\\)\nBut since it’s not clear how to introduce gradient for \\(sign\\), as well as how to perform updated over quantized weights - they use non-quantized weights and perform quantization as a forward pass.\nBut there is a method - https://arxiv.org/pdf/2307.05695.pdf paper introduce a ReLoRA approach - making a high-rank updates via a sequence of low-rank ones.\nSo I am gonna try the following approach:\n\nInitialize \\(W\\) weights\nSave them to a temporary file \\(File_W\\)\nQuantize \\(W\\) weights to \\(W_quant\\)\nUnload \\(W\\) weights\nFor a few iterations\n\nTrain \\(i\\)-th LoRA adapter via ReLoRA procedure\nLoad \\(W\\) weights from \\(File_W\\)\nMerge with \\(\\Delta W\\) we got from the adapter: \\(W = W + \\Delta W\\)\nSave them to a temporary file \\(File_W\\)\nQuantize \\(W\\) weights to \\(W_quant\\)\nUnload \\(W\\) weights",
    "crumbs": [
      "bitlinear"
    ]
  },
  {
    "objectID": "bitlinear.html#implementation",
    "href": "bitlinear.html#implementation",
    "title": "bitlinear",
    "section": "Implementation",
    "text": "Implementation\n\nimport tempfile",
    "crumbs": [
      "bitlinear"
    ]
  },
  {
    "objectID": "bitlinear.html#determining-parameter-count-per-byte",
    "href": "bitlinear.html#determining-parameter-count-per-byte",
    "title": "bitlinear",
    "section": "Determining parameter count per byte",
    "text": "Determining parameter count per byte\nOkay, keeping in mind we need each parameter to have 3 values - {-1, 0, 1} * scale - let’s see how much parameters can we pack inside one uint32\nSo 5 parameters group per one uint8.",
    "crumbs": [
      "bitlinear"
    ]
  },
  {
    "objectID": "bitlinear.html#parameter-group-index-to-parameter-mapping",
    "href": "bitlinear.html#parameter-group-index-to-parameter-mapping",
    "title": "bitlinear",
    "section": "Parameter group index to parameter mapping",
    "text": "Parameter group index to parameter mapping\nNow let’s generate a tensor of uint8 index -&gt; [5 * float16] values\n\nMAPPING_UINT8_TO_5_PARAMS\n\ntensor([[-1., -1., -1., -1., -1.],\n        [-1., -1., -1., -1.,  0.],\n        [-1., -1., -1., -1.,  1.],\n        ...,\n        [ 1.,  1.,  1.,  1.,  1.],\n        [ 1.,  1.,  1.,  1.,  1.],\n        [ 1.,  1.,  1.,  1.,  1.]])",
    "crumbs": [
      "bitlinear"
    ]
  },
  {
    "objectID": "bitlinear.html#reverse-mapping",
    "href": "bitlinear.html#reverse-mapping",
    "title": "bitlinear",
    "section": "Reverse mapping",
    "text": "Reverse mapping\nNow we also need a reverse mapping from N-parameter group to grop index.\n\n\nquantization_inner\n\n quantization_inner (data:numpy.ndarray)",
    "crumbs": [
      "bitlinear"
    ]
  },
  {
    "objectID": "bitlinear.html#dequantization-function",
    "href": "bitlinear.html#dequantization-function",
    "title": "bitlinear",
    "section": "Dequantization function",
    "text": "Dequantization function\nDequantization is a fairly simple process - for each byte here - use it as an index for the mapping\n\n\ndequantize_weights\n\n dequantize_weights (weight_mapping:torch.Tensor,\n                     packed_weights:torch.Tensor,\n                     scale:Union[torch.Tensor,float])",
    "crumbs": [
      "bitlinear"
    ]
  },
  {
    "objectID": "bitlinear.html#quantization",
    "href": "bitlinear.html#quantization",
    "title": "bitlinear",
    "section": "Quantization",
    "text": "Quantization\nReverse quantization is a bit more complicated.\nHere we calculate the difference between each 5-parameter group vector ang pick the most similar vector index\n\n\nquantize_weights\n\n quantize_weights (weights:torch.FloatTensor,\n                   mean:Union[torch.Tensor,float])",
    "crumbs": [
      "bitlinear"
    ]
  },
  {
    "objectID": "bitlinear.html#test-dequantization-quantization-sequence",
    "href": "bitlinear.html#test-dequantization-quantization-sequence",
    "title": "bitlinear",
    "section": "Test dequantization-quantization sequence",
    "text": "Test dequantization-quantization sequence\nTo test this two functions above I will generate some random “packed weights” than dequantize them and quantize back.\n\ndef test_dequantize_quantize():\n    torch.manual_seed(42)\n    for _ in range(100):\n        index = (torch.rand(10, 200 // STORAGE_VALUES_PER_ITEM) * (3 ** STORAGE_VALUES_PER_ITEM - 1)).round()\n\n        weights_dequant = dequantize_weights(MAPPING_UINT8_TO_5_PARAMS, index, 50.0)\n        index_restored = quantize_weights(weights_dequant, 0)\n\n        assert (index == index_restored).all()\n\n\ntest_dequantize_quantize()\n\n\n\nDequantizeApply\n\n DequantizeApply (*args, **kwargs)\n\nBase class to create custom autograd.Function.\nTo create a custom autograd.Function, subclass this class and implement the :meth:forward and :meth:backward static methods. Then, to use your custom op in the forward pass, call the class method apply. Do not call :meth:forward directly.\nTo ensure correctness and best performance, make sure you are calling the correct methods on ctx and validating your backward function using :func:torch.autograd.gradcheck.\nSee :ref:extending-autograd for more details on how to use this class.\nExamples::\n&gt;&gt;&gt; # xdoctest: +REQUIRES(env:TORCH_DOCTEST_AUTOGRAD)\n&gt;&gt;&gt; class Exp(Function):\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def forward(ctx, i):\n&gt;&gt;&gt;         result = i.exp()\n&gt;&gt;&gt;         ctx.save_for_backward(result)\n&gt;&gt;&gt;         return result\n&gt;&gt;&gt;\n&gt;&gt;&gt;     @staticmethod\n&gt;&gt;&gt;     def backward(ctx, grad_output):\n&gt;&gt;&gt;         result, = ctx.saved_tensors\n&gt;&gt;&gt;         return grad_output * result\n&gt;&gt;&gt;\n&gt;&gt;&gt; # Use it by calling the apply method:\n&gt;&gt;&gt; # xdoctest: +SKIP\n&gt;&gt;&gt; output = Exp.apply(input)\n\n\n\nBitLinear\n\n BitLinear (in_features:int, out_features:int, bias:bool=True,\n            device=None, dtype=None,\n            original_weights_filename:Optional[str]=None,\n            adapter:Optional[bitlinear.adapters.LinearAdapter]=None,\n            initial_linear:Optional[torch.nn.modules.linear.Linear]=None)\n\nBase class for all neural network modules.\nYour models should also subclass this class.\nModules can also contain other Modules, allowing to nest them in a tree structure. You can assign the submodules as regular attributes::\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5)\n        self.conv2 = nn.Conv2d(20, 20, 5)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        return F.relu(self.conv2(x))\nSubmodules assigned in this way will be registered, and will have their parameters converted too when you call :meth:to, etc.\n.. note:: As per the example above, an __init__() call to the parent class must be made before assignment on the child.\n:ivar training: Boolean represents whether this module is in training or evaluation mode. :vartype training: bool\n\ndef test_lora_merging():\n    with tempfile.NamedTemporaryFile() as weights_file:\n        weights_file_name = weights_file.name\n        torch.manual_seed(42)\n        similarities_diff_lora_merged_and_lora_raw = []\n        for batch_size in range(1, 1000):\n            # Initialize \"linear\" layer\n            linear = BitLinear(\n                in_features=10,\n                out_features=20,\n                bias=True,\n                device=None,\n                dtype=None,\n                original_weights_filename=weights_file_name,\n                adapter=None,\n            )\n            # Compute layer output for random input\n            input = torch.rand(batch_size, 10)\n            output_linear = linear(input)\n            # Add non-trained (means lora_B is all zeroes) adapter\n            lora = LoRAAdapter(in_features=10, out_features=20, lora_rank=3)\n            linear.adapter = lora\n            output_linear_nottrained_lora = linear(input)\n\n            # Check that a non-trained adapter do not change a thing\n            similarity_raw_and_resetted = torch.nn.functional.cosine_similarity(output_linear, output_linear_nottrained_lora)\n            assert similarity_raw_and_resetted.min() &gt;= 1.0 - 1e-5\n\n            # Imitate training making some lora_B nonzero\n            lora.lora_b.data = torch.rand(*lora.lora_b.shape) * 10.0\n            # Compute layer+lora output\n            output_linear_lora_trained = linear(input)\n\n            # Merge lora adapter into weights and re-quantize them\n            linear.merge_adapter()\n            # Compute merged layer+lora output\n            output_linear_lora_merged = linear(input)\n\n            # Check that merged layer+lora is more similar to layer+lora than to raw layer output\n            similarity_lora_and_raw = torch.nn.functional.cosine_similarity(output_linear, output_linear_lora_trained)\n            similarity_lora_and_merged = torch.nn.functional.cosine_similarity(output_linear_lora_trained, output_linear_lora_merged)\n\n            similarities_diff_lora_merged_and_lora_raw.append(\n                (similarity_lora_and_merged - similarity_lora_and_raw).mean().item()\n            )\n        assert torch.FloatTensor(similarities_diff_lora_merged_and_lora_raw).mean() &gt; 0.3\n\n\ntest_lora_merging()",
    "crumbs": [
      "bitlinear"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "BitLinear",
    "section": "",
    "text": "I just did not come up with some better naming, lol.\nThis repository contains my experimental package where I: - Were inspired by “The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits” approach\nSo I made a custom linear layer which:\nAnd ReLoRA training is basically the same as in the corresponding paper (except for the way we merge model and LoRA’s).\nNow I am conducting experiments to see how well does the approach works.",
    "crumbs": [
      "BitLinear"
    ]
  },
  {
    "objectID": "index.html#install",
    "href": "index.html#install",
    "title": "BitLinear",
    "section": "Install",
    "text": "Install\npip install bitlinear@git+https://github.com/alex4321/bitlinear.git",
    "crumbs": [
      "BitLinear"
    ]
  },
  {
    "objectID": "index.html#experiment-results",
    "href": "index.html#experiment-results",
    "title": "BitLinear",
    "section": "Experiment results",
    "text": "Experiment results\n\nPretraining\n\nI conducted two experiments raw Mistral architecture model pretraining and BitNet+ReLoRA Mistral architecture model pretraining\nand results end up being comparable.\nFor the original method I got\nStep    Training Loss   Validation Loss Memory Usage Mb\n2000    5.178500    4.655300    29107\n4000    4.267300    4.253386    29541\n6000    3.997500    4.030161    30021\n8000    4.739200    3.861828    30521\n10000   3.159500    3.761141    30521\n12000   4.065400    3.672445    30521\n14000   3.764200    3.598749    30521\n16000   3.897100    3.530349    30521\n18000   3.261500    3.468710    30521\n20000   2.736200    3.411213    30521\n22000   2.150800    3.359339    30521\n24000   2.949400    3.317924    30521\nwhile for optimized one:\nStep    Training Loss   Validation Loss Memory Usage Mb\n2000    5.049300    4.500881    8928\n4000    4.113500    4.085669    8840\n6000    3.948200    3.910413    8636\n8000    4.600700    3.776079    10518\n10000   3.124400    3.722620    9386\n12000   4.122400    3.669651    8794\n14000   3.781300    3.606225    8486\n16000   3.858200    3.570461    8912\n18000   3.319800    3.529242    8826\n20000   2.763000    3.487872    9796\n22000   2.137100    3.442672    9616\n24000   3.097400    3.421924    8994\n-\n26000   2.706200    3.380465    9468\n28000   3.897200    3.357405    10138\n30000   3.217000    3.332875    9786",
    "crumbs": [
      "BitLinear"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "BitLinear",
    "section": "How to use",
    "text": "How to use\nHere I will make a simple example upon which I am experimenting now (there is still some debugging)\nimport os\nimport datasets\nfrom transformers import AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer, \\\n    TrainerCallback\nfrom transformers.models.mistral import MistralConfig\nfrom bitlinear.adapters import LoRAAdapter\nfrom bitlinear.models.mistral import BitMistralForCausalLM\nfrom bitlinear.relora import ReLoRAOptimizer, ReLoRASchedulerLambda\nfrom torch.optim import AdamW\nfrom torch.optim.lr_scheduler import LambdaLR\nimport subprocess\nimport torch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nSTORE_DIR = \"StoredWeights\"\n\nos.makedirs(STORE_DIR, exist_ok=True)\n\nconfig = MistralConfig(\n    vocab_size=32000,\n    hidden_size=4160, # Original Mistral have 4090, this is closest multiplier for both 5 and 32\n    intermediate_size=14400, # Original Mistral have 14336, this is closest multiplier for both 5 and 32\n    num_hidden_layers=5, # Instead of 32 - to make model roughly 1-billion params\n    num_attention_heads=32,\n    num_key_value_heads=8,\n    hidden_act=\"silu\",\n    max_position_embeddings=32768,\n    initializer_range=0.02,\n    rms_norm_eps=1e-5,\n    use_cache=True,\n    rope_theta=10000.0,\n    sliding_window=4096,\n    attention_dropout=0.0,\n)\nmodel = BitMistralForCausalLM(\n    config=config,\n    fname_prefix=f\"{STORE_DIR}/bitmistal\"\n).to(\"cuda:0\")\n\nmodel.add_adapters(\n    LoRAAdapter,\n    {\n        \"lora_rank\": 128,\n    }\n)\n\noptimizer = ReLoRAOptimizer(\n    model.parameters(),\n    model.mergeable_layers(),\n    optimizer_cls=AdamW,\n    optimizer_params={},\n    reset_n_steps=500,\n    lr=1e-5,\n)\nlr_scheduler = LambdaLR(\n    optimizer,\n    ReLoRASchedulerLambda(\n        lr_lambda=lambda step: step / 1000 if step &lt; 1000 else min(1.0 - (step - 50000) / 50000, 1e-5),\n        warmup_n_steps=100,\n        reset_n_steps=500,\n    )\n)\n\noptimizer = ReLoRAOptimizer(\n    model.parameters(),\n    model.mergeable_layers(),\n    optimizer_cls=AdamW,\n    optimizer_params={},\n    reset_n_steps=500,\n    lr=1e-5,\n)\nlr_scheduler = LambdaLR(\n    optimizer,\n    ReLoRASchedulerLambda(\n        lr_lambda=lambda step: step / 1000 if step &lt; 1000 else min(1.0 - (step - 50000) / 50000, 1e-5),\n        warmup_n_steps=100,\n        reset_n_steps=500,\n    )\n)\n\ndef tokenize_function(examples):\n    return tokenizer(examples[\"text\"], truncation=True, max_length=1024)\n\n# Tokenize all parts of the dataset\ntokenized_datasets = dataset_text.map(tokenize_function, batched=True, remove_columns=[\"text\"])\ntokenized_datasets\n\ndata_collator = DataCollatorForLanguageModeling(tokenizer, \n                                                mlm=False,\n                                                pad_to_multiple_of=8)\n\nclass GpuMemoryLoggingCallback(TrainerCallback):\n    \"\"\"A custom callback for logging GPU memory usage.\"\"\"\n    \n    def on_log(self, args, state, control, logs=None, **kwargs):\n        # Check if CUDA is available to avoid errors on CPU-only environments\n        if torch.cuda.is_available():\n            # Assuming a single-GPU setup here; adjust for multi-GPU as needed\n            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n                                    capture_output=True, text=True)\n            memory_usage = result.stdout.strip()\n            \n            # Convert memory usage to an integer (MB) and log it\n            logs['gpu_memory_usage_mb'] = int(memory_usage)\n        else:\n            logs['gpu_memory_usage_mb'] = 0  # Default to 0 if not using GPU\n\nmodel.train()\nmodel.gradient_checkpointing_enable()\ntraining_args = TrainingArguments(\n    output_dir=\"./mistral-2b-stored-model\",\n    per_device_train_batch_size=4,\n    gradient_accumulation_steps=4,\n    per_device_eval_batch_size=4,\n    eval_accumulation_steps=4,\n    logging_dir=\"./mistral-2b-tensorboard-bitwise\",\n    logging_steps=1,\n    save_strategy=\"steps\",\n    save_steps=2000,\n    evaluation_strategy=\"steps\",\n    eval_steps=2000,\n    fp16=True,\n    gradient_checkpointing=True,\n    report_to=\"tensorboard\",\n    max_steps=10000,\n    # No need to specify data collator here, it's passed to the Trainer constructor\n)\n\n# Initialize the Trainer with the data collator\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=tokenized_datasets[\"train\"],  # Assuming these are ready; dynamically tokenized if not\n    eval_dataset=tokenized_datasets[\"validation\"],\n    data_collator=data_collator,\n    optimizers=(optimizer, lr_scheduler),\n    callbacks=[GpuMemoryLoggingCallback()],\n)\n\n# Train\ntrainer.train()",
    "crumbs": [
      "BitLinear"
    ]
  },
  {
    "objectID": "index.html#todo",
    "href": "index.html#todo",
    "title": "BitLinear",
    "section": "TODO",
    "text": "TODO\n\nMake experiment with finetuning the existing model this way\nMake experiment with self-distillation from the existing model this way\nWrite an optimized BitLinear kernel:\n\ncurrent one dequantize weights than feed them to torch.nn.function.linear so spawning dequantized weights in memory will take some time. Why not do matrix multiplication on the fly, this way further decrease an amount of memory required for both training and inference?",
    "crumbs": [
      "BitLinear"
    ]
  },
  {
    "objectID": "relora.html",
    "href": "relora.html",
    "title": "ReLoRA BitNet training",
    "section": "",
    "text": "The key idea of ReLoRA ( https://arxiv.org/pdf/2307.05695.pdf ) is to make a high-rank updates as a series of low-rank updates.\nFor that they introduce a training procedure where - they add LoRA adapters to the linear layers of the original network - each N steps they: - merge LoRA back in the original layer - reset LoRA adapter state - reset optimizer state - reset LR to zero to make some quick warmup\nIn my case I will apply LoRA to the BitNet-approach quantized weights during training, while during merging I will apply them to the original weights (and quantize them back later). This will introduce quality loss, but it’s yet to see how much for big networks.",
    "crumbs": [
      "ReLoRA BitNet training"
    ]
  },
  {
    "objectID": "relora.html#idea",
    "href": "relora.html#idea",
    "title": "ReLoRA BitNet training",
    "section": "",
    "text": "The key idea of ReLoRA ( https://arxiv.org/pdf/2307.05695.pdf ) is to make a high-rank updates as a series of low-rank updates.\nFor that they introduce a training procedure where - they add LoRA adapters to the linear layers of the original network - each N steps they: - merge LoRA back in the original layer - reset LoRA adapter state - reset optimizer state - reset LR to zero to make some quick warmup\nIn my case I will apply LoRA to the BitNet-approach quantized weights during training, while during merging I will apply them to the original weights (and quantize them back later). This will introduce quality loss, but it’s yet to see how much for big networks.",
    "crumbs": [
      "ReLoRA BitNet training"
    ]
  },
  {
    "objectID": "relora.html#implementation",
    "href": "relora.html#implementation",
    "title": "ReLoRA BitNet training",
    "section": "Implementation",
    "text": "Implementation\n\nimport matplotlib.pyplot as plt\nimport tempfile\n\n\nReLoRA style Optimizer\nThis “optimizer” will: - use some other optimizer inside (see optimizer_cls / optimizer_params / lr) - reset it every reset_n_steps steps - call merge every reset_n_steps steps (see mergeable_layers) - also will be a proxy for a few inner optimizer things necessary to work properly\n\n\n\nReLoRAOptimizer\n\n ReLoRAOptimizer\n                  (params:Union[Iterable[torch.Tensor],Iterable[Dict[str,A\n                  ny]]], mergeable_layers:Iterable[bitlinear.adapters.Merg\n                  eableLayer],\n                  optimizer_cls:Type[torch.optim.optimizer.Optimizer],\n                  optimizer_params:Dict[str,Any], reset_n_steps:int,\n                  lr:float=0.001)\n\nBase class for all optimizers.\n.. warning:: Parameters need to be specified as collections that have a deterministic ordering that is consistent between runs. Examples of objects that don’t satisfy those properties are sets and iterators over values of dictionaries.\nArgs: params (iterable): an iterable of :class:torch.Tensor s or :class:dict s. Specifies what Tensors should be optimized. defaults: (dict): a dict containing default values of optimization options (used when a parameter group doesn’t specify them).\n\n\nReLoRA style LR scheduler\nThis callable will:\n\nuse some lr_lambda inside it for a factual schedule\nbut on top of that make some quick (warmup_n_steps) warmups every reset_n_steps steps\n\n\n\n\nis_pickleable\n\n is_pickleable (obj:Any)\n\n\n\n\nReLoRASchedulerLambda\n\n ReLoRASchedulerLambda (lr_lambda:&lt;built-infunctioncallable&gt;,\n                        reset_n_steps:int, warmup_n_steps:int)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\nLinearWarmupSchedule\n\n LinearWarmupSchedule (warmup_steps:int, total_steps:int)\n\nInitialize self. See help(type(self)) for accurate signature.\n\ndef test_linear_schedule_with_resets():\n    scheduler = ReLoRASchedulerLambda(\n        LinearWarmupSchedule(1000, 5000),\n        reset_n_steps=500,\n        warmup_n_steps=100,\n    )\n    plt.plot([\n        scheduler(i)\n        for i in range(6000)\n    ])\n    plt.title(\"ReLoRA LR schedule (on top of LinearWarmup)\")\n    plt.grid()\n\n\ntest_linear_schedule_with_resets()\n\n\n\n\n\n\n\n\n\n\nTest\nFor test I will just make a XOR network and compare the way it works with a basic FP32 multilayer perceptron.\n\nDataset preparation\n\ntorch.manual_seed(42)\nx_orig = torch.Tensor([\n    [0, 0],\n    [0, 1],\n    [1, 0],\n    [1, 1],\n])\ny_orig = torch.FloatTensor([\n    [0],\n    [1],\n    [1],\n    [0],\n])\nx = []\ny = []\nfor i in range(500000):\n    x.append(x_orig + torch.rand(size=x_orig.shape) * 0.2 - 0.1)\n    y.append(y_orig)\nx = torch.cat(x, dim=0)\ny = torch.cat(y, dim=0)\ndataset = TensorDataset(x, y)\n\n\n\nBitNet ReLoRA XOR\n\ndef test_training(device):\n    torch.manual_seed(42)\n    with tempfile.NamedTemporaryFile() as weights_temp_file_0, \\\n        tempfile.NamedTemporaryFile() as weights_temp_file_1:\n        weights_temp_file_0_name = weights_temp_file_0.name\n        weights_temp_file_1_name = weights_temp_file_1.name\n        dataloader = DataLoader(dataset, batch_size=100)\n        dataloader_iter = iter(dataloader)\n\n        layer_0 = BitLinear(\n            in_features=2,\n            out_features=10,\n            bias=True,\n            device=device,\n            original_weights_filename=weights_temp_file_0_name,\n            adapter=None,\n        )\n        lora_0 = LoRAAdapter(\n            in_features=layer_0.padded_in_features,\n            out_features=layer_0.padded_out_features,\n            lora_rank=2,\n            device=device,\n        )\n        layer_0.adapter = lora_0\n\n        layer_1 = BitLinear(\n            in_features=10,\n            out_features=1,\n            device=device,\n            original_weights_filename=weights_temp_file_1_name,\n            adapter=None,\n        )\n        lora_1 = LoRAAdapter(\n            in_features=layer_1.padded_in_features,\n            out_features=layer_1.padded_out_features,\n            lora_rank=2,\n            device=device,\n        )\n        layer_1.adapter = lora_1\n\n        network = torch.nn.Sequential(\n            layer_0,\n            torch.nn.GELU(),\n            layer_1,\n            torch.nn.Sigmoid()\n        )\n        loss = torch.nn.BCELoss()\n\n        losses = []\n        optimizer = ReLoRAOptimizer(\n            network.parameters(),\n            mergeable_layers=[\n                layer_0, layer_1,\n            ],\n            optimizer_cls=Adam,\n            optimizer_params={},\n            reset_n_steps=250,\n            lr=1e-3,\n        )\n        lr_scheduler = LambdaLR(\n            optimizer,\n            ReLoRASchedulerLambda(\n                LinearWarmupSchedule(250, 5000),\n                reset_n_steps=250,\n                warmup_n_steps=25,\n            ),\n        )\n\n        network.train()\n        lrs = []\n        for _ in range(5000):\n            optimizer.zero_grad()\n            batch = next(dataloader_iter)\n            x, y = batch\n            x_moved = x.to(device)\n            y_pred = network(x_moved)\n            loss_value = loss(y_pred, y.to(device))\n            loss_value.backward()\n            losses.append(loss_value.item())\n            optimizer.step()\n            lr_scheduler.step()\n            lr = optimizer.state_dict()['param_groups'][0]['lr']\n            lrs.append(lr)\n        \n        assert loss_value.item() &lt; 0.2, f\"Expected loss&lt;0.2, got {loss_value}\"\n\n        plt.plot(losses)\n        plt.ylabel(\"Loss\")\n        plt.xlabel(\"Steps\")\n        plt.title(\"1.5bit ReLoRA XOR loss\")\n        plt.grid()\n        plt.show()\n\n        plt.plot(lrs)\n        plt.ylabel(\"LR\")\n        plt.xlabel(\"Steps\")\n        plt.title(\"1.5bit ReLoRA XOR LR\")\n        plt.grid()\n        plt.show()\n\n\ntest_training(\"cpu\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nif torch.cuda.is_available():\n    test_training(\"cuda:0\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHere: - we see time was quite long (9.6 sec CPU, 11 sec GPU) - yet as the further profiling will show it’s ReLoRA resets which consume much time, so perhaps it will be lesser part of the process than updates themselves for bigger network - we see quality losses every 250 steps which must be expected - every time I apply LoRA to the original weights, not the quantized it were trained for, and quantize it once again\n\n\nXOR based on nn.Linear\n\ndef test_normal(device):\n    torch.manual_seed(42)\n    dataloader = DataLoader(dataset, batch_size=100)\n    dataloader_iter = iter(dataloader)\n\n    layer_0 = torch.nn.Linear(\n        in_features=2,\n        out_features=10,\n        bias=True,\n        device=device,\n    )\n    layer_1 = torch.nn.Linear(\n        in_features=10,\n        out_features=1,\n        bias=True,\n        device=device,\n    )\n    network = torch.nn.Sequential(\n        layer_0,\n        torch.nn.GELU(),\n        layer_1,\n        torch.nn.Sigmoid()\n    )\n    loss = torch.nn.BCELoss()\n\n    losses = []\n    optimizer = Adam(\n        network.parameters(),\n        lr=1e-3,\n    )\n    lr_scheduler = LambdaLR(\n        optimizer,\n        LinearWarmupSchedule(250, 5000),\n    )\n    network.train()\n    lrs = []\n    for _ in range(5000):\n        optimizer.zero_grad()\n        batch = next(dataloader_iter)\n        x, y = batch\n        y_pred = network(x.to(device))\n        loss_value = loss(y_pred, y.to(device))\n        loss_value.backward()\n        losses.append(loss_value.item())\n        optimizer.step()\n        lr_scheduler.step()\n        lr = optimizer.state_dict()['param_groups'][0]['lr']\n        lrs.append(lr)\n        \n    assert loss_value.item() &lt; 0.2, f\"Expected loss&lt;0.2, got {loss_value}\"\n\n    plt.plot(losses)\n    plt.ylabel(\"Loss\")\n    plt.xlabel(\"Steps\")\n    plt.title(\"XOR loss\")\n    plt.grid()\n    plt.show()\n\n    plt.plot(lrs)\n    plt.ylabel(\"LR\")\n    plt.xlabel(\"Steps\")\n    plt.title(\"XOR LR\")\n    plt.grid()\n    plt.show()\n\n\ntest_normal(\"cpu\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nif torch.cuda.is_available():\n    test_normal(\"cuda:0\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSo it surely comes to an optimal state faster with such a small network. Let’s see bigger ones in the further work.\n\n\nCUDA+autocast\n\ndef test_training_cuda_autocast():\n    device = \"cuda:0\"\n    torch.manual_seed(42)\n    with tempfile.NamedTemporaryFile() as weights_temp_file_0, \\\n        tempfile.NamedTemporaryFile() as weights_temp_file_1:\n        weights_temp_file_0_name = weights_temp_file_0.name\n        weights_temp_file_1_name = weights_temp_file_1.name\n        dataloader = DataLoader(dataset, batch_size=100)\n        dataloader_iter = iter(dataloader)\n\n        layer_0 = BitLinear(\n            in_features=2,\n            out_features=10,\n            bias=True,\n            device=device,\n            original_weights_filename=weights_temp_file_0_name,\n            adapter=None,\n        )\n        lora_0 = LoRAAdapter(\n            in_features=layer_0.padded_in_features,\n            out_features=layer_0.padded_out_features,\n            lora_rank=2,\n            device=device,\n        )\n        layer_0.adapter = lora_0\n\n        layer_1 = BitLinear(\n            in_features=10,\n            out_features=1,\n            device=device,\n            original_weights_filename=weights_temp_file_1_name,\n            adapter=None,\n        )\n        lora_1 = LoRAAdapter(\n            in_features=layer_1.padded_in_features,\n            out_features=layer_1.padded_out_features,\n            lora_rank=2,\n            device=device,\n        )\n        layer_1.adapter = lora_1\n        network = torch.nn.Sequential(\n            layer_0,\n            torch.nn.GELU(),\n            layer_1,\n        )\n        loss = torch.nn.BCEWithLogitsLoss()\n\n        losses = []\n        optimizer = ReLoRAOptimizer(\n            network.parameters(),\n            mergeable_layers=[\n                layer_0, layer_1,\n            ],\n            optimizer_cls=Adam,\n            optimizer_params={},\n            reset_n_steps=250,\n            lr=1e-3,\n        )\n        lr_scheduler = LambdaLR(\n            optimizer,\n            ReLoRASchedulerLambda(\n                LinearWarmupSchedule(250, 5000),\n                reset_n_steps=250,\n                warmup_n_steps=25,\n            ),\n        )\n        network.train()\n        scaler = torch.cuda.amp.GradScaler()\n        lrs = []\n        for _ in range(5000):\n            optimizer.zero_grad()\n            batch = next(dataloader_iter)\n            x, y = batch\n            with torch.cuda.amp.autocast(enabled=True):\n                y_pred = network(x.to(device))\n                loss_value = loss(y_pred, y.to(device))\n            \n            losses.append(loss_value.item())\n            \n            scaler.scale(loss_value).backward()\n            \n            scaler.step(optimizer)\n            scaler.update()\n\n            lr_scheduler.step()\n            lr = optimizer.state_dict()['param_groups'][0]['lr']\n            lrs.append(lr)\n        \n        assert loss_value.item() &lt; 0.2, f\"Expected loss&lt;0.2, got {loss_value}\"\n\n        plt.plot(losses)\n        plt.ylabel(\"Loss\")\n        plt.xlabel(\"Steps\")\n        plt.title(\"1.5bit ReLoRA XOR loss\")\n        plt.grid()\n        plt.show()\n\n        plt.plot(lrs)\n        plt.ylabel(\"LR\")\n        plt.xlabel(\"Steps\")\n        plt.title(\"1.5bit ReLoRA XOR LR\")\n        plt.grid()\n        plt.show()\n\n\nif torch.cuda.is_available():\n    test_training_cuda_autocast()\n\nRuntimeError: expected mat1 and mat2 to have the same dtype, but got: c10::Half != float\n\n\nOkay, looks fine\n\n\n\nLoading from a pre-trained model\n\ndef test_loadingnormal(device):\n    def basic_network():\n        torch.manual_seed(42)\n        dataloader = DataLoader(dataset, batch_size=100)\n        dataloader_iter = iter(dataloader)\n        layer_0 = torch.nn.Linear(\n            in_features=2,\n            out_features=10,\n            bias=True,\n            device=device,\n        )\n        layer_1 = torch.nn.Linear(\n            in_features=10,\n            out_features=1,\n            bias=True,\n            device=device,\n        )\n        network = torch.nn.Sequential(\n            layer_0,\n            torch.nn.GELU(),\n            layer_1,\n            torch.nn.Sigmoid()\n        )\n        loss = torch.nn.BCELoss()\n\n        losses = []\n        optimizer = Adam(\n            network.parameters(),\n            lr=1e-3,\n        )\n        lr_scheduler = LambdaLR(\n            optimizer,\n            LinearWarmupSchedule(250, 5000),\n        )\n        network.train()\n        lrs = []\n        for _ in range(5000):\n            optimizer.zero_grad()\n            batch = next(dataloader_iter)\n            x, y = batch\n            y_pred = network(x.to(device))\n            loss_value = loss(y_pred, y.to(device))\n            loss_value.backward()\n            losses.append(loss_value.item())\n            optimizer.step()\n            lr_scheduler.step()\n            lr = optimizer.state_dict()['param_groups'][0]['lr']\n            lrs.append(lr)\n            \n        return layer_0, layer_1\n    \n    def relora_network(layer_0_basic, layer_1_basic):\n        torch.manual_seed(42)\n        dataloader = DataLoader(dataset, batch_size=100)\n        dataloader_iter = iter(dataloader)\n        \n        with tempfile.NamedTemporaryFile() as weights_temp_file_0, \\\n            tempfile.NamedTemporaryFile() as weights_temp_file_1:\n            weights_temp_file_0_name = weights_temp_file_0.name\n            weights_temp_file_1_name = weights_temp_file_1.name\n            dataloader = DataLoader(dataset, batch_size=100)\n            dataloader_iter = iter(dataloader)\n\n            layer_0 = BitLinear(\n                in_features=2,\n                out_features=10,\n                bias=True,\n                device=device,\n                original_weights_filename=weights_temp_file_0_name,\n                adapter=None,\n                initial_linear=layer_0_basic,\n            )\n            lora_0 = LoRAAdapter(\n                in_features=layer_0.padded_in_features,\n                out_features=layer_0.padded_out_features,\n                lora_rank=2,\n                device=device,\n            )\n            layer_0.adapter = lora_0\n\n            layer_1 = BitLinear(\n                in_features=10,\n                out_features=1,\n                device=device,\n                original_weights_filename=weights_temp_file_1_name,\n                adapter=None,\n                initial_linear=layer_1_basic,\n            )\n            lora_1 = LoRAAdapter(\n                in_features=layer_1.padded_in_features,\n                out_features=layer_1.padded_out_features,\n                lora_rank=2,\n                device=device,\n            )\n            layer_1.adapter = lora_1\n\n            network = torch.nn.Sequential(\n                layer_0,\n                torch.nn.GELU(),\n                layer_1,\n                torch.nn.Sigmoid()\n            )\n            loss = torch.nn.BCELoss()\n\n            losses = []\n            optimizer = ReLoRAOptimizer(\n                network.parameters(),\n                mergeable_layers=[\n                    layer_0, layer_1,\n                ],\n                optimizer_cls=Adam,\n                optimizer_params={},\n                reset_n_steps=250,\n                lr=1e-3,\n            )\n            lr_scheduler = LambdaLR(\n                optimizer,\n                ReLoRASchedulerLambda(\n                    LinearWarmupSchedule(250, 5000),\n                    reset_n_steps=250,\n                    warmup_n_steps=25,\n                ),\n            )\n\n            network.train()\n            lrs = []\n            for _ in range(5000):\n                optimizer.zero_grad()\n                batch = next(dataloader_iter)\n                x, y = batch\n                x_moved = x.to(device)\n                y_pred = network(x_moved)\n                loss_value = loss(y_pred, y.to(device))\n                loss_value.backward()\n                losses.append(loss_value.item())\n                optimizer.step()\n                lr_scheduler.step()\n                lr = optimizer.state_dict()['param_groups'][0]['lr']\n                lrs.append(lr)\n            \n            assert loss_value.item() &lt; 0.2, f\"Expected loss&lt;0.2, got {loss_value}\"\n\n            plt.plot(losses)\n            plt.ylabel(\"Loss\")\n            plt.xlabel(\"Steps\")\n            plt.title(\"1.5bit ReLoRA XOR loss\")\n            plt.grid()\n            plt.show()\n\n            plt.plot(lrs)\n            plt.ylabel(\"LR\")\n            plt.xlabel(\"Steps\")\n            plt.title(\"1.5bit ReLoRA XOR LR\")\n            plt.grid()\n            plt.show()\n    \n    relora_network(\n        *basic_network()\n    )\n\n\ntest_loadingnormal(\"cpu\")",
    "crumbs": [
      "ReLoRA BitNet training"
    ]
  }
]