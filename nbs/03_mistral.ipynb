{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp models.mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\"\"\"\n",
    "Modified code for Mistral model\n",
    "\"\"\"\n",
    "from typing import List, Optional, Type, Any, Dict\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch import nn\n",
    "\n",
    "from transformers.activations import ACT2FN\n",
    "from transformers.models.mistral.configuration_mistral import MistralConfig\n",
    "\n",
    "from transformers.models.mistral.modeling_mistral import \\\n",
    "    is_flash_attn_2_available, is_flash_attn_greater_or_equal_2_10, \\\n",
    "    MistralRMSNorm, \\\n",
    "    MistralRotaryEmbedding, \\\n",
    "    MistralMLP, MistralAttention, \\\n",
    "    MistralFlashAttention2, \\\n",
    "    MistralSdpaAttention, \\\n",
    "    MistralDecoderLayer, \\\n",
    "    MistralPreTrainedModel, \\\n",
    "    MistralModel, \\\n",
    "    MistralForCausalLM, \\\n",
    "    MistralForSequenceClassification\n",
    "\n",
    "from bitlinear.bitlinear import BitLinear\n",
    "from bitlinear.adapters import LinearAdapter, LoRAAdapter, MergeableLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BitMistralMLP(MistralMLP):\n",
    "    def __init__(self, config: MistralConfig, fname_prefix: str):\n",
    "        nn.Module.__init__(self)\n",
    "        self.config = config\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.intermediate_size = config.intermediate_size\n",
    "        self.gate_proj = BitLinear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.intermediate_size,\n",
    "            bias=False,\n",
    "            original_weights_filename=f\"{fname_prefix}-gate-proj.bin\",\n",
    "        )\n",
    "        self.up_proj = BitLinear(\n",
    "            in_features=self.hidden_size,\n",
    "            out_features=self.intermediate_size,\n",
    "            bias=False,\n",
    "            original_weights_filename=f\"{fname_prefix}-up-proj.bin\",\n",
    "        )\n",
    "        self.down_proj = BitLinear(\n",
    "            in_features=self.intermediate_size,\n",
    "            out_features=self.hidden_size,\n",
    "            bias=False,\n",
    "            original_weights_filename=f\"{fname_prefix}-down-proj.bin\"\n",
    "        )\n",
    "        self.act_fn = ACT2FN[config.hidden_act]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BitMistralAttentionBase:\n",
    "    def __init__(self, config: MistralConfig, fname_prefix: str, layer_idx: Optional[int] = None):\n",
    "        nn.Module.__init__(self)\n",
    "        self.config = config\n",
    "        self.layer_idx = layer_idx\n",
    "        assert layer_idx is not None\n",
    "\n",
    "        self.hidden_size = config.hidden_size\n",
    "        self.num_heads = config.num_attention_heads\n",
    "        self.head_dim = self.hidden_size // self.num_heads\n",
    "        self.num_key_value_heads = config.num_key_value_heads\n",
    "        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n",
    "        self.max_position_embeddings = config.max_position_embeddings\n",
    "        self.rope_theta = config.rope_theta\n",
    "        self.is_causal = True\n",
    "        self.attention_dropout = config.attention_dropout\n",
    "\n",
    "        if (self.head_dim * self.num_heads) != self.hidden_size:\n",
    "            raise ValueError(\n",
    "                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n",
    "                f\" and `num_heads`: {self.num_heads}).\"\n",
    "            )\n",
    "        self.q_proj = BitLinear(\n",
    "            self.hidden_size,\n",
    "            self.num_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            original_weights_filename=f\"{fname_prefix}-q-proj.bin\"\n",
    "        )\n",
    "        self.k_proj = BitLinear(\n",
    "            self.hidden_size,\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            original_weights_filename=f\"{fname_prefix}-k-proj.bin\"\n",
    "        )\n",
    "        self.v_proj = BitLinear(\n",
    "            self.hidden_size,\n",
    "            self.num_key_value_heads * self.head_dim,\n",
    "            bias=False,\n",
    "            original_weights_filename=f\"{fname_prefix}-v-proj.bin\"\n",
    "        )\n",
    "        self.o_proj = BitLinear(\n",
    "            self.num_heads * self.head_dim,\n",
    "            self.hidden_size,\n",
    "            bias=False,\n",
    "            original_weights_filename=f\"{fname_prefix}-o-proj.bin\"\n",
    "        )\n",
    "\n",
    "        self.rotary_emb = MistralRotaryEmbedding(\n",
    "            self.head_dim,\n",
    "            max_position_embeddings=self.max_position_embeddings,\n",
    "            base=self.rope_theta,\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "class BitMistralAttention(MistralAttention, BitMistralAttentionBase):\n",
    "    def __init__(self, config: MistralConfig, fname_prefix: str, layer_idx: Optional[int] = None):\n",
    "        BitMistralAttentionBase.__init__(self, config, fname_prefix, layer_idx)\n",
    "\n",
    "\n",
    "class BitMistralFlashAttention2(MistralFlashAttention2, BitMistralAttentionBase):\n",
    "    def __init__(self, config: MistralConfig, fname_prefix: str, layer_idx: Optional[int] = None):\n",
    "        BitMistralAttentionBase.__init__(self, config, fname_prefix, layer_idx)\n",
    "        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n",
    "\n",
    "\n",
    "class BitMistralSdpaAttention(MistralSdpaAttention, BitMistralAttentionBase):\n",
    "    def __init__(self, config: MistralConfig, fname_prefix: str, layer_idx: Optional[int] = None):\n",
    "        BitMistralAttentionBase.__init__(self, config, fname_prefix, layer_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "BITMISTRAL_ATTENTION_CLASSES = {\n",
    "    \"eager\": BitMistralAttention,\n",
    "    \"flash_attention_2\": BitMistralFlashAttention2,\n",
    "    \"sdpa\": BitMistralSdpaAttention,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BitMistralDecoderLayer(MistralDecoderLayer):\n",
    "    def __init__(self, config: MistralConfig, layer_idx: int, fname_prefix: str):\n",
    "        nn.Module.__init__(self)\n",
    "        self.hidden_size = config.hidden_size\n",
    "\n",
    "        self.layer_idx = layer_idx\n",
    "        self.self_attn = BITMISTRAL_ATTENTION_CLASSES[config._attn_implementation](\n",
    "            config=config,\n",
    "            fname_prefix=f\"{fname_prefix}-self-attn.bin\",\n",
    "            layer_idx=layer_idx,\n",
    "        )\n",
    "        self.mlp = BitMistralMLP(\n",
    "            config=config,\n",
    "            fname_prefix=f\"{fname_prefix}-mlp.bin\",\n",
    "        )\n",
    "        self.input_layernorm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "        self.post_attention_layernorm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BitMistralPreTrainedModel(MistralPreTrainedModel):\n",
    "    def _init_weights(self, module):\n",
    "        std = self.config.initializer_range\n",
    "        if isinstance(module, nn.Linear):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        if isinstance(module, BitLinear):\n",
    "            module.update_weights(\n",
    "                torch.normal(\n",
    "                    mean=torch.zeros(module.out_features, module.in_features),\n",
    "                    std=torch.ones(module.out_features, module.in_features) * std,\n",
    "                )\n",
    "            )\n",
    "            if module.bias is not None:\n",
    "                module.bias.zero_()\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            module.weight.data.normal_(mean=0.0, std=std)\n",
    "            if module.padding_idx is not None:\n",
    "                module.weight.data[module.padding_idx].zero_()\n",
    "    \n",
    "\n",
    "class BitMistralAdaptersMixin(nn.Module):\n",
    "    def _get_bitlinear_layers(self) -> List[BitLinear]:\n",
    "        layers = []\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, BitLinear):\n",
    "                layers.append(layer)\n",
    "        return layers\n",
    "    \n",
    "    def add_adapters(self, adapter_type: Type[LinearAdapter], params: Dict[str, Any]) -> List[LinearAdapter]:\n",
    "        layers = self._get_bitlinear_layers()\n",
    "        adapters = []\n",
    "        for layer in layers:\n",
    "            layer_params = dict(**params)\n",
    "            layer_params[\"in_features\"] = layer.in_features\n",
    "            layer_params[\"out_features\"] = layer.out_features\n",
    "            layer_params[\"device\"] = layer.quant_weight.device\n",
    "            adapter = adapter_type(**layer_params)\n",
    "            layer.adapter = adapter\n",
    "            adapters.append(adapter)\n",
    "        return adapters\n",
    "    \n",
    "    def remove_adapters(self) -> None:\n",
    "        layers = self._get_bitlinear_layers()\n",
    "        for layer in layers:\n",
    "            if layer.adapter is not None:\n",
    "                layer.adapter = None\n",
    "\n",
    "\n",
    "    def mergeable_layers(self) -> List[MergeableLayer]:\n",
    "        layers = []\n",
    "        for layer in self.modules():\n",
    "            if isinstance(layer, MergeableLayer):\n",
    "                layers.append(layer)\n",
    "        return layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BitMistralModel(MistralModel, BitMistralAdaptersMixin):\n",
    "    def __init__(self, config: MistralConfig, fname_prefix: str):\n",
    "        BitMistralPreTrainedModel.__init__(self, config)\n",
    "        self.padding_idx = config.pad_token_id\n",
    "        self.vocab_size = config.vocab_size\n",
    "\n",
    "        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n",
    "        self.layers = nn.ModuleList(\n",
    "            [\n",
    "                BitMistralDecoderLayer(config, layer_idx, f\"{fname_prefix}-decoder-{layer_idx}\")\n",
    "                for layer_idx in range(config.num_hidden_layers)\n",
    "            ]\n",
    "        )\n",
    "        self._attn_implementation = config._attn_implementation\n",
    "        self.norm = MistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n",
    "\n",
    "        self.gradient_checkpointing = False\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BitMistralForCausalLM(MistralForCausalLM, BitMistralAdaptersMixin):\n",
    "    def __init__(self, config: MistralConfig, fname_prefix: str):\n",
    "        BitMistralPreTrainedModel.__init__(self, config)\n",
    "        self.model = BitMistralModel(config, fname_prefix)\n",
    "        self.vocab_size = config.vocab_size\n",
    "        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BitMistralForSequenceClassification(MistralForSequenceClassification, BitMistralAdaptersMixin):\n",
    "    def __init__(self, config: MistralConfig, fname_prefix: str):\n",
    "        BitMistralPreTrainedModel.__init__(self, config)\n",
    "        self.num_labels = config.num_labels\n",
    "        self.model = MistralModel(config, fname_prefix)\n",
    "        self.score = nn.Linear(config.hidden_size, self.num_labels, bias=False)\n",
    "\n",
    "        # Initialize weights and apply final processing\n",
    "        self.post_init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
