{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp adapters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# adapters\n",
    "\n",
    "> Since the training method I am going to check with my technique (ReLoRA) is about using low-rank adapters to make high-rank updates (alongside with a specific quantization method) I will make a LoRA implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "Basically the idea of low-rank adapters is the following one:\n",
    "\n",
    "$\n",
    "Linear(W, x) = W x\n",
    "$\n",
    "where $W$ is $(outputFeatures, inputFeatures)$ matrix and $x$ is $(inputFeatures, batchSize)$ matrix\n",
    "\n",
    "So if W_changed is $W + \\Delta W$:\n",
    "\n",
    "$\n",
    "Linear(W_changed, x) = W_changed x = (W + \\Delta W) x = W x + \\Delta W x\n",
    "$\n",
    "\n",
    "And if $\\Delta W$ is a low-rank matrix than we can represent it as \n",
    "\n",
    "$\n",
    "\\Delta W = lora_B lora_A\n",
    "$\n",
    "\n",
    "Where $lora_B$ is $(outputFeatures, loraRank)$ matrix and $lora_A$ is $(loraRank, inputFeatures)$ matrix\n",
    "\n",
    "So\n",
    "\n",
    "$\n",
    "Linear(W_changed, x) = W x + (lora_B lora_A) x\n",
    "$\n",
    "\n",
    "But since $\\Delta W$ matrix itself is relatively big and the matrix multiplication is an associative operation - we can make it the following way:\n",
    "\n",
    "$\n",
    "Linear(W_changed, x) = W x + lora_B (lora_A x)\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LinearAdapter(torch.nn.Module):\n",
    "    def reset(self):\n",
    "        \"\"\"\n",
    "        Reset adapter (so it actually will not influence the output)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def calculate_weight_update(self):\n",
    "        \"\"\"\n",
    "        Calculate $\\Delta W$ matrix for the current adapter state\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LoRAAdapter(LinearAdapter):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 lora_rank: int,\n",
    "                 device=None):\n",
    "        super(LoRAAdapter, self).__init__()\n",
    "        self.lora_a = torch.nn.Parameter(torch.zeros(lora_rank, in_features, device=device))\n",
    "        self.lora_b = torch.nn.Parameter(torch.zeros(out_features, lora_rank, device=device))\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self) -> None:\n",
    "        torch.nn.init.xavier_uniform_(self.lora_a.data)\n",
    "        torch.nn.init.zeros_(self.lora_b.data)\n",
    "        self.lora_a.data.requires_grad = True\n",
    "        self.lora_b.data.requires_grad = True\n",
    "    \n",
    "    def calculate_weight_update(self):\n",
    "        return self.lora_b.matmul(self.lora_a)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.nn.functional.linear(\n",
    "            torch.nn.functional.linear(input, self.lora_a),\n",
    "            self.lora_b\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MergeableLayer(torch.nn.Module):\n",
    "    def __init__(self, adapter: LinearAdapter) -> None:\n",
    "        super(MergeableLayer, self).__init__()\n",
    "        self.adapter = adapter\n",
    "    \n",
    "    def merge_adapter(self) -> None:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
