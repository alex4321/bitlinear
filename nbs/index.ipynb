{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BitLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just did not come up with some better naming, lol.\n",
    "\n",
    "This repository contains my experimental package where I:\n",
    "- Were inspired by [\"The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits\"](https://arxiv.org/abs/2402.17764) approach\n",
    "  \n",
    "  - Essentially guys have proven that $log_2(3)$ bits of *informational capacity* per parameter is enought to pretrain a language model (they have shown that we can make a linear layer operating *factual* weights having only three values $(-scale, 0, scale)$).\n",
    "  \n",
    "  - However since they used, well, fully-gradiental method to train LLM - in the reality weights were 16/32 bit, and were quantized to these values on the fly by some (approximately) differentiable quantization function\n",
    "  \n",
    "  - By the way it also means that if model have $|W|$ trainable weights - optimizing through an optimizer like Adam will consume $3|W|dataTypeBytes$ parameters \n",
    "\n",
    "- On the other hand in [\"ReLoRA: High-Rank Training Through Low-Rank Updates\"](https://arxiv.org/abs/2307.05695) other researchers show that if we freeze the original model and do incremental procedure of \"train LoRA adapters - merge them into the original model - reset optimizer state\" - the quality becomes comparable with standart training approach\n",
    "  \n",
    "  - So the gradient updates do not apply to the original model weights here, means we:\n",
    "    \n",
    "    - Need only $3(|A|+|B|)dataTypeBytes + |W|dataTypeBytes$ bytes of memory used by the optimization process (where $|A|$ and $|B|$ is an summary amount of parameters in $LoraA$ / $LoraB$ accross the whole model)\n",
    "    \n",
    "    - However the original model need to be stored in memory fully\n",
    "\n",
    "So I made a custom linear layer which:\n",
    "\n",
    "- Do not store original weights for long - only save to file, quantize it and than store quantized weights inside itself (so each byte represents a group of 5 parameters)\n",
    "  \n",
    "  - So $ W_{quant} = quantize(W) $\n",
    "  \n",
    "- Do dequantization during a forward pass and adds adapter as well\n",
    "  \n",
    "  - $ dequantize(W_{quant}) x + bias + LoRA(x) $\n",
    "\n",
    "- Can merge LoRA adapter inside itself\n",
    "  \n",
    "  - To do so it:\n",
    "    \n",
    "    - Load the file with a previously saved weights\n",
    "    \n",
    "    - Merge LoRA's $\\Delta W$:\n",
    "      $ W = W + \\Delta W $\n",
    "    \n",
    "    - Save these new weights\n",
    "    \n",
    "    - Quantize it $ W_{quant} = quantize(W) $\n",
    "  \n",
    "  - Surely with such a procedure we *must* except some quality loss:\n",
    "    \n",
    "    - At first - LoRA were trained to work upon quantized weights, which is a mere approximation of the original ones\n",
    "    \n",
    "    - At second - LoRA-introduced update than becomes quantized again\n",
    "\n",
    "And ReLoRA training is basically the same as in the corresponding paper (except for the way we merge model and LoRA's).\n",
    "\n",
    "Now I am conducting experiments to see how well does the approach works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sh\n",
    "pip install bitlinear@git+https://github.com/alex4321/bitlinear.git\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment results\n",
    "\n",
    "- Pretraining\n",
    "\n",
    "I conducted two experiments [raw Mistral architecture model pretraining](experiments/00_pretraining/mistral-training.ipynb) \n",
    "and [BitNet+ReLoRA Mistral architecture model pretraining](experiments/00_pretraining/bitmistral-training-lr-2e-4-rank-128--2000-restart.ipynb)\n",
    "\n",
    "and results end up being comparable.\n",
    "\n",
    "For the original method I got\n",
    "```\n",
    "Step\tTraining Loss\tValidation Loss\tMemory Usage Mb\n",
    "2000\t5.178500\t4.655300\t29107\n",
    "4000\t4.267300\t4.253386\t29541\n",
    "6000\t3.997500\t4.030161\t30021\n",
    "8000\t4.739200\t3.861828\t30521\n",
    "10000\t3.159500\t3.761141\t30521\n",
    "12000\t4.065400\t3.672445\t30521\n",
    "14000\t3.764200\t3.598749\t30521\n",
    "16000\t3.897100\t3.530349\t30521\n",
    "18000\t3.261500\t3.468710\t30521\n",
    "20000\t2.736200\t3.411213\t30521\n",
    "22000\t2.150800\t3.359339\t30521\n",
    "24000\t2.949400\t3.317924\t30521\n",
    "```\n",
    "while for optimized one:\n",
    "```\n",
    "Step\tTraining Loss\tValidation Loss\tMemory Usage Mb\n",
    "2000\t5.049300\t4.500881\t8928\n",
    "4000\t4.113500\t4.085669\t8840\n",
    "6000\t3.948200\t3.910413\t8636\n",
    "8000\t4.600700\t3.776079\t10518\n",
    "10000\t3.124400\t3.722620\t9386\n",
    "12000\t4.122400\t3.669651\t8794\n",
    "14000\t3.781300\t3.606225\t8486\n",
    "16000\t3.858200\t3.570461\t8912\n",
    "18000\t3.319800\t3.529242\t8826\n",
    "20000\t2.763000\t3.487872\t9796\n",
    "22000\t2.137100\t3.442672\t9616\n",
    "24000\t3.097400\t3.421924\t8994\n",
    "-\n",
    "26000\t2.706200\t3.380465\t9468\n",
    "28000\t3.897200\t3.357405\t10138\n",
    "30000\t3.217000\t3.332875\t9786\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I will make a simple example upon which I am experimenting now (there is still some debugging)\n",
    "\n",
    "```python\n",
    "import os\n",
    "import datasets\n",
    "from transformers import AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer, \\\n",
    "    TrainerCallback\n",
    "from transformers.models.mistral import MistralConfig\n",
    "from bitlinear.adapters import LoRAAdapter\n",
    "from bitlinear.models.mistral import BitMistralForCausalLM\n",
    "from bitlinear.relora import ReLoRAOptimizer, ReLoRASchedulerLambda\n",
    "from torch.optim import AdamW\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "import subprocess\n",
    "import torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "STORE_DIR = \"StoredWeights\"\n",
    "\n",
    "os.makedirs(STORE_DIR, exist_ok=True)\n",
    "\n",
    "config = MistralConfig(\n",
    "    vocab_size=32000,\n",
    "    hidden_size=4160, # Original Mistral have 4090, this is closest multiplier for both 5 and 32\n",
    "    intermediate_size=14400, # Original Mistral have 14336, this is closest multiplier for both 5 and 32\n",
    "    num_hidden_layers=5, # Instead of 32 - to make model roughly 1-billion params\n",
    "    num_attention_heads=32,\n",
    "    num_key_value_heads=8,\n",
    "    hidden_act=\"silu\",\n",
    "    max_position_embeddings=32768,\n",
    "    initializer_range=0.02,\n",
    "    rms_norm_eps=1e-5,\n",
    "    use_cache=True,\n",
    "    rope_theta=10000.0,\n",
    "    sliding_window=4096,\n",
    "    attention_dropout=0.0,\n",
    ")\n",
    "model = BitMistralForCausalLM(\n",
    "    config=config,\n",
    "    fname_prefix=f\"{STORE_DIR}/bitmistal\"\n",
    ").to(\"cuda:0\")\n",
    "\n",
    "model.add_adapters(\n",
    "    LoRAAdapter,\n",
    "    {\n",
    "        \"lora_rank\": 128,\n",
    "    }\n",
    ")\n",
    "\n",
    "optimizer = ReLoRAOptimizer(\n",
    "    model.parameters(),\n",
    "    model.mergeable_layers(),\n",
    "    optimizer_cls=AdamW,\n",
    "    optimizer_params={},\n",
    "    reset_n_steps=500,\n",
    "    lr=1e-5,\n",
    ")\n",
    "lr_scheduler = LambdaLR(\n",
    "    optimizer,\n",
    "    ReLoRASchedulerLambda(\n",
    "        lr_lambda=lambda step: step / 1000 if step < 1000 else min(1.0 - (step - 50000) / 50000, 1e-5),\n",
    "        warmup_n_steps=100,\n",
    "        reset_n_steps=500,\n",
    "    )\n",
    ")\n",
    "\n",
    "optimizer = ReLoRAOptimizer(\n",
    "    model.parameters(),\n",
    "    model.mergeable_layers(),\n",
    "    optimizer_cls=AdamW,\n",
    "    optimizer_params={},\n",
    "    reset_n_steps=500,\n",
    "    lr=1e-5,\n",
    ")\n",
    "lr_scheduler = LambdaLR(\n",
    "    optimizer,\n",
    "    ReLoRASchedulerLambda(\n",
    "        lr_lambda=lambda step: step / 1000 if step < 1000 else min(1.0 - (step - 50000) / 50000, 1e-5),\n",
    "        warmup_n_steps=100,\n",
    "        reset_n_steps=500,\n",
    "    )\n",
    ")\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, max_length=1024)\n",
    "\n",
    "# Tokenize all parts of the dataset\n",
    "tokenized_datasets = dataset_text.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "tokenized_datasets\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, \n",
    "                                                mlm=False,\n",
    "                                                pad_to_multiple_of=8)\n",
    "\n",
    "class GpuMemoryLoggingCallback(TrainerCallback):\n",
    "    \"\"\"A custom callback for logging GPU memory usage.\"\"\"\n",
    "    \n",
    "    def on_log(self, args, state, control, logs=None, **kwargs):\n",
    "        # Check if CUDA is available to avoid errors on CPU-only environments\n",
    "        if torch.cuda.is_available():\n",
    "            # Assuming a single-GPU setup here; adjust for multi-GPU as needed\n",
    "            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],\n",
    "                                    capture_output=True, text=True)\n",
    "            memory_usage = result.stdout.strip()\n",
    "            \n",
    "            # Convert memory usage to an integer (MB) and log it\n",
    "            logs['gpu_memory_usage_mb'] = int(memory_usage)\n",
    "        else:\n",
    "            logs['gpu_memory_usage_mb'] = 0  # Default to 0 if not using GPU\n",
    "\n",
    "model.train()\n",
    "model.gradient_checkpointing_enable()\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./mistral-2b-stored-model\",\n",
    "    per_device_train_batch_size=4,\n",
    "    gradient_accumulation_steps=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    eval_accumulation_steps=4,\n",
    "    logging_dir=\"./mistral-2b-tensorboard-bitwise\",\n",
    "    logging_steps=1,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=2000,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=2000,\n",
    "    fp16=True,\n",
    "    gradient_checkpointing=True,\n",
    "    report_to=\"tensorboard\",\n",
    "    max_steps=10000,\n",
    "    # No need to specify data collator here, it's passed to the Trainer constructor\n",
    ")\n",
    "\n",
    "# Initialize the Trainer with the data collator\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],  # Assuming these are ready; dynamically tokenized if not\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    optimizers=(optimizer, lr_scheduler),\n",
    "    callbacks=[GpuMemoryLoggingCallback()],\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "- Make experiment with finetuning the existing model this way\n",
    "- Make experiment with self-distillation from the existing model this way\n",
    "- Write an optimized BitLinear kernel:\n",
    "  - current one dequantize weights than feed them to `torch.nn.function.linear` so spawning dequantized weights in memory will take some time. Why not do matrix multiplication on the fly, this way further decrease an amount of memory required for both training and inference?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
