{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# bitlinear\n",
    "\n",
    "> A quantized linear layer implementation here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Idea\n",
    "\n",
    "This paper https://arxiv.org/pdf/2402.17764.pdf suggest that we can use a linear layer which weight can effectively only get three values $(-scale, 0, scale)$\n",
    "\n",
    "But for pretraining purpose they still use full (or half) precision weights, effectively do quantization only during the forward pass:\n",
    "\n",
    "$\n",
    "ThreeValuesLinear(W, x) = ThreeValuesWeight(W) x\n",
    "$\n",
    "\n",
    "$\n",
    "ThreeValuesWeight(W) = \n",
    "    sign({\n",
    "        W - {\\sum(W) \\over {inputFeatures outputFeatures}}\n",
    "    })\n",
    "    {\n",
    "        {|W|} \\over {inputFeatures outputFeatures}\n",
    "    }\n",
    "$\n",
    "\n",
    "So $sign(...)$ part effectively make it $(-1, 0, 1)$ values, and multiplying to a $scale$ converts to $(-scale, 0, scale)$\n",
    "\n",
    "But since it's not clear how to introduce gradient for $sign$, as well as how to perform updated over quantized weights - they use non-quantized weights and perform quantization as a forward pass.\n",
    "\n",
    "But there is a method - https://arxiv.org/pdf/2307.05695.pdf paper introduce a ReLoRA approach - making a high-rank updates via a sequence of low-rank ones.\n",
    "\n",
    "So I am gonna try the following approach:\n",
    "\n",
    "- Initialize $W$ weights\n",
    "- Save them to a temporary file $File_W$\n",
    "- Quantize $W$ weights to $W_quant$\n",
    "- Unload $W$ weights\n",
    "- For a few iterations\n",
    "  - Train $i$-th LoRA adapter via ReLoRA procedure\n",
    "  - Load $W$ weights from $File_W$\n",
    "  - Merge with $\\Delta W$ we got from the adapter: $W = W + \\Delta W$\n",
    "  - Save them to a temporary file $File_W$\n",
    "  - Quantize $W$ weights to $W_quant$\n",
    "  - Unload $W$ weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp bitlinear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from typing import List, Union, Tuple, Iterable\n",
    "import torch\n",
    "from bitlinear.adapters import LinearAdapter, LoRAAdapter, MergeableLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining parameter count per byte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, keeping in mind we need each parameter to have 3 values - {-1, 0, 1} * scale - let's see how much parameters can we pack inside one uint32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "STORAGE_BIT_COUNT = 8\n",
    "STORAGE_DTYPE = torch.ByteTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| export\n",
    "def _get_parameter_count_per_n_bits(n: int) -> int:\n",
    "    i = 0\n",
    "    while True:\n",
    "        j = i + 1\n",
    "        if 3 ** j > 2 ** n:\n",
    "            break\n",
    "        i += 1\n",
    "    return i\n",
    "\n",
    "\n",
    "STORAGE_VALUES_PER_ITEM = _get_parameter_count_per_n_bits(STORAGE_BIT_COUNT)\n",
    "STORAGE_VALUES_PER_ITEM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So 5 parameters group per one uint8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter group index to parameter mapping\n",
    "\n",
    "Now let's generate a tensor of uint8 index -> [5 * float16] values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _generate_parameter_mappings(parameter_count: int, pad_to_size: int) -> List[List[int]]:\n",
    "    def _iter(rest_count):\n",
    "        if rest_count == 0:\n",
    "            return [[]]\n",
    "        else:\n",
    "            result = []\n",
    "            for p in [-1, 0, 1]:\n",
    "                for rest in _iter(rest_count-1):\n",
    "                    result.append([p] + rest)\n",
    "            return result\n",
    "    \n",
    "    response = _iter(parameter_count)\n",
    "    assert len(response) < pad_to_size\n",
    "    response += [ [1] * parameter_count ] * (pad_to_size - len(response))\n",
    "    return response\n",
    "\n",
    "\n",
    "def _generate_parameter_mappings_tensor(parameter_count: int, pad_to_size: int) -> torch.Tensor:\n",
    "    return torch.FloatTensor(_generate_parameter_mappings(parameter_count, pad_to_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "MAPPING_UINT8_TO_5_PARAMS = _generate_parameter_mappings_tensor(\n",
    "    STORAGE_VALUES_PER_ITEM,\n",
    "    2 ** STORAGE_BIT_COUNT\n",
    ")\n",
    "assert MAPPING_UINT8_TO_5_PARAMS.shape == (256, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -1., -1., -1., -1.],\n",
       "        [-1., -1., -1., -1.,  0.],\n",
       "        [-1., -1., -1., -1.,  1.],\n",
       "        ...,\n",
       "        [ 1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.],\n",
       "        [ 1.,  1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAPPING_UINT8_TO_5_PARAMS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dequantization function\n",
    "\n",
    "Dequantization is a fairly simple process - for each byte here - use it as an index for the mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad\n",
    "def dequantize_weights(weight_mapping: torch.Tensor, packed_weights: torch.Tensor, scale: Union[torch.Tensor, float]) \\\n",
    "    -> torch.Tensor:\n",
    "    weights_per_item = weight_mapping.shape[-1]\n",
    "    weights_packed_shape = list(packed_weights.shape[:-1]) + \\\n",
    "        [weights_per_item * packed_weights.shape[-1]]\n",
    "    dequantized_weights_k = weight_mapping[packed_weights.long(), :].view(weights_packed_shape)\n",
    "    return dequantized_weights_k * scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "Reverse quantization is a bit more complicated.\n",
    "\n",
    "Here we calculate the difference between each 5-parameter group vector ang pick the most similar vector index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@torch.no_grad\n",
    "def quantize_weights(weight_mapping: torch.FloatTensor, weights: torch.FloatTensor, mean: Union[torch.Tensor, float]) \\\n",
    "    -> torch.Tensor:\n",
    "    weights_centered = weights - mean\n",
    "    weights_sign = weights_centered.sign()\n",
    "    weights_reshaped = weights_sign.view(\n",
    "        list(weights_sign.shape[:-1]) + [weights_sign.shape[-1] // STORAGE_VALUES_PER_ITEM, 1, STORAGE_VALUES_PER_ITEM]\n",
    "    )\n",
    "    weight_mapping_reshaped = weight_mapping.view(\n",
    "        [1] * len(weights_sign.shape) + [2 ** STORAGE_BIT_COUNT, STORAGE_VALUES_PER_ITEM]\n",
    "    )\n",
    "    weights_mapping_diff = (weights_reshaped - weight_mapping_reshaped).abs()\n",
    "    weights_group_scores = weights_mapping_diff.sum(dim=-1)\n",
    "    weigths_group_chosen = weights_group_scores.argmin(dim=-1)\n",
    "    weigths_group_chosen.clamp_(0, 3 ** STORAGE_VALUES_PER_ITEM - 1)\n",
    "    return weigths_group_chosen.byte()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test dequantization-quantization sequence\n",
    "\n",
    "To test this two functions above I will generate some random \"packed weights\" than dequantize them and quantize back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_dequantize_quantize():\n",
    "    torch.manual_seed(42)\n",
    "    for _ in range(100):\n",
    "        index = (torch.rand(10, 200 // STORAGE_VALUES_PER_ITEM) * (3 ** STORAGE_VALUES_PER_ITEM - 1)).round()\n",
    "        index_restored = quantize_weights(\n",
    "            MAPPING_UINT8_TO_5_PARAMS,\n",
    "            dequantize_weights(MAPPING_UINT8_TO_5_PARAMS, index, 50.0),\n",
    "            0\n",
    "        )\n",
    "        assert (index == index_restored).all()\n",
    "\n",
    "\n",
    "test_dequantize_quantize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BitLinear(MergeableLayer):\n",
    "    def __init__(self,\n",
    "                 in_features: int,\n",
    "                 out_features: int,\n",
    "                 bias: bool = True,\n",
    "                 device=None,\n",
    "                 dtype=None,\n",
    "                 original_weights_filename: Union[str, None] = None,\n",
    "                 adapter: Union[None, LinearAdapter]=None):\n",
    "        super(BitLinear, self).__init__(adapter=adapter)\n",
    "        assert in_features % STORAGE_VALUES_PER_ITEM == 0\n",
    "\n",
    "        if device:\n",
    "            self.mapping = MAPPING_UINT8_TO_5_PARAMS.to(device=device)\n",
    "        else:\n",
    "            self.mapping = MAPPING_UINT8_TO_5_PARAMS * 1\n",
    "        self.mapping_cpu = MAPPING_UINT8_TO_5_PARAMS * 1\n",
    "            \n",
    "\n",
    "        self.original_weights_filename = original_weights_filename\n",
    "        \n",
    "        initial_linear = torch.nn.Linear(in_features, out_features, bias=bias, device=\"cpu\", dtype=dtype)\n",
    "        self.mean, self.scale, self.quant_weight = self._wrap_parameters(self._quantize_weight(\n",
    "            initial_linear.weight,\n",
    "            device=device\n",
    "        ))\n",
    "        \n",
    "        if bias:\n",
    "            bias_tensor = initial_linear.bias.data\n",
    "            if device is not None:\n",
    "                bias_tensor = bias_tensor.to(device)\n",
    "            self.bias = torch.nn.Parameter(bias_tensor)\n",
    "        else:\n",
    "            self.register_parameter(\"bias\", None)\n",
    "        if original_weights_filename:\n",
    "            torch.save(\n",
    "                initial_linear.weight,\n",
    "                self.original_weights_filename,\n",
    "            )        \n",
    "        \n",
    "        self.adapter = adapter\n",
    "\n",
    "    def _wrap_parameters(self, tensors: Iterable[torch.Tensor]) -> List[torch.Tensor]:\n",
    "        return [\n",
    "            torch.nn.Parameter(tensor, requires_grad=False)\n",
    "            for tensor in tensors\n",
    "        ]\n",
    "\n",
    "    @torch.no_grad\n",
    "    def _quantize_weight(self, weight: torch.Tensor, device: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        weight = weight.cpu()\n",
    "        mean = weight.mean().cpu()\n",
    "        scale = weight.abs().mean().cpu()\n",
    "        qweight = quantize_weights(self.mapping_cpu, weight, mean).to(device)\n",
    "        return mean.to(device), scale.to(device), qweight.to(device)\n",
    "\n",
    "    def forward(self, input: torch.Tensor) -> torch.Tensor:\n",
    "        W = dequantize_weights(self.mapping, self.quant_weight, self.scale)\n",
    "        response = torch.nn.functional.linear(input, W, self.bias)\n",
    "        if self.adapter:\n",
    "            adapter = self.adapter(input)\n",
    "            response = response + adapter\n",
    "        return response\n",
    "    \n",
    "    def _update_parameter(self, parameters: Iterable[torch.nn.Parameter], tensors: Iterable[torch.Tensor]) -> None:\n",
    "        for parameter, tensor in zip(parameters, tensors):\n",
    "            parameter.data = tensor\n",
    "    \n",
    "    @torch.no_grad\n",
    "    def _update_weight(self, update: torch.Tensor) -> None:\n",
    "        assert self.original_weights_filename is not None\n",
    "        weight = torch.load(\n",
    "            self.original_weights_filename,\n",
    "            map_location=\"cpu\"\n",
    "        )\n",
    "        weight_updated = weight + update.to(\"cpu\")\n",
    "        self._update_parameter(\n",
    "            [self.mean, self.scale, self.quant_weight],\n",
    "            self._quantize_weight(weight_updated, self.quant_weight.device)\n",
    "        )\n",
    "        torch.save(\n",
    "            weight_updated,\n",
    "            self.original_weights_filename,\n",
    "        )\n",
    "\n",
    "    def merge_adapter(self) -> None:\n",
    "        assert self.adapter is not None\n",
    "        self._update_weight(\n",
    "            self.adapter.calculate_weight_update()\n",
    "        )\n",
    "        self.adapter.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_lora_merging():\n",
    "    torch.manual_seed(42)\n",
    "    similarities_diff_lora_merged_and_lora_raw = []\n",
    "    for batch_size in range(1, 1000):\n",
    "        # Initialize \"linear\" layer\n",
    "        linear = BitLinear(\n",
    "            in_features=10,\n",
    "            out_features=20,\n",
    "            bias=True,\n",
    "            device=None,\n",
    "            dtype=None,\n",
    "            original_weights_filename=\"test-linear-weights.bin\",\n",
    "            adapter=None,\n",
    "        )\n",
    "        # Compute layer output for random input\n",
    "        input = torch.rand(batch_size, 10)\n",
    "        output_linear = linear(input)\n",
    "        # Add non-trained (means lora_B is all zeroes) adapter\n",
    "        lora = LoRAAdapter(in_features=10, out_features=20, lora_rank=3)\n",
    "        linear.adapter = lora\n",
    "        output_linear_nottrained_lora = linear(input)\n",
    "\n",
    "        # Check that a non-trained adapter do not change a thing\n",
    "        similarity_raw_and_resetted = torch.nn.functional.cosine_similarity(output_linear, output_linear_nottrained_lora)\n",
    "        assert similarity_raw_and_resetted.min() >= 1.0 - 1e-5\n",
    "\n",
    "        # Imitate training making some lora_B nonzero\n",
    "        lora.lora_b.data = torch.rand(*lora.lora_b.shape) * 10.0\n",
    "        # Compute layer+lora output\n",
    "        output_linear_lora_trained = linear(input)\n",
    "\n",
    "        # Merge lora adapter into weights and re-quantize them\n",
    "        delta_W = lora.calculate_weight_update()\n",
    "        linear.merge_adapter()\n",
    "        # Compute merged layer+lora output\n",
    "        output_linear_lora_merged = linear(input)\n",
    "\n",
    "        # Check that merged layer+lora is more similar to layer+lora than to raw layer output\n",
    "        similarity_lora_and_raw = torch.nn.functional.cosine_similarity(output_linear, output_linear_lora_trained)\n",
    "        similarity_lora_and_merged = torch.nn.functional.cosine_similarity(output_linear_lora_trained, output_linear_lora_merged)\n",
    "\n",
    "        similarities_diff_lora_merged_and_lora_raw.append(\n",
    "            (similarity_lora_and_merged - similarity_lora_and_raw).mean().item()\n",
    "        )\n",
    "    assert torch.FloatTensor(similarities_diff_lora_merged_and_lora_raw).mean() > 0.3\n",
    "\n",
    "\n",
    "test_lora_merging()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
