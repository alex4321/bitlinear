# BitLinear


<!-- WARNING: THIS FILE WAS AUTOGENERATED! DO NOT EDIT! -->

I just did not come up with some better naming, lol.

This repository contains my experimental package where I: - Were
inspired by [“The Era of 1-bit LLMs: All Large Language Models are in
1.58 Bits”](https://arxiv.org/abs/2402.17764) approach

- Essentially guys have proven that $log_2(3)$ bits of *informational
  capacity* per parameter is enought to pretrain a language model (they
  have shown that we can make a linear layer operating *factual* weights
  having only three values $(-scale, 0, scale)$).

- However since they used, well, fully-gradiental method to train LLM -
  in the reality weights were 16/32 bit, and were quantized to these
  values on the fly by some (approximately) differentiable quantization
  function

- By the way it also means that if model have $|W|$ trainable weights -
  optimizing through an optimizer like Adam will consume
  $3|W|dataTypeBytes$ parameters

- On the other hand in [“ReLoRA: High-Rank Training Through Low-Rank
  Updates”](https://arxiv.org/abs/2307.05695) other researchers show
  that if we freeze the original model and do incremental procedure of
  “train LoRA adapters - merge them into the original model - reset
  optimizer state” - the quality becomes comparable with standart
  training approach

  - So the gradient updates do not apply to the original model weights
    here, means we:

    - Need only $3(|A|+|B|)dataTypeBytes + |W|dataTypeBytes$ bytes of
      memory used by the optimization process (where $|A|$ and $|B|$ is
      an summary amount of parameters in $LoraA$ / $LoraB$ accross the
      whole model)

    - However the original model need to be stored in memory fully

So I made a custom linear layer which:

- Do not store original weights for long - only save to file, quantize
  it and than store quantized weights inside itself (so each byte
  represents a group of 5 parameters)

  - So \$ W\_{quant} = quantize(W) \$

- Do dequantization during a forward pass and adds adapter as well

  - \$ dequantize(W\_{quant}) x + bias + LoRA(x) \$

- Can merge LoRA adapter inside itself

  - To do so it:

    - Load the file with a previously saved weights

    - Merge LoRA’s $\Delta W$: \$ W = W + W \$

    - Save these new weights

    - Quantize it \$ W\_{quant} = quantize(W) \$

  - Surely with such a procedure we *must* except some quality loss:

    - At first - LoRA were trained to work upon quantized weights, which
      is a mere approximation of the original ones

    - At second - LoRA-introduced update than becomes quantized again

And ReLoRA training is basically the same as in the corresponding paper
(except for the way we merge model and LoRA’s).

Now I am conducting experiments to see how well does the approach works.

## Install

``` sh
pip install bitlinear@git+https://github.com/alex4321/bitlinear.git
```

## How to use

Here I will make a simple example upon which I am experimenting now
(there is still some debugging)

``` python
import os
import datasets
from transformers import AutoTokenizer, DataCollatorForLanguageModeling, TrainingArguments, Trainer, \
    TrainerCallback
from transformers.models.mistral import MistralConfig
from bitlinear.adapters import LoRAAdapter
from bitlinear.models.mistral import BitMistralForCausalLM
from bitlinear.relora import ReLoRAOptimizer, ReLoRASchedulerLambda
from torch.optim import AdamW
from torch.optim.lr_scheduler import LambdaLR
import subprocess
import torch

import warnings
warnings.filterwarnings("ignore")

STORE_DIR = "StoredWeights"

os.makedirs(STORE_DIR, exist_ok=True)

config = MistralConfig(
    vocab_size=32000,
    hidden_size=4160, # Original Mistral have 4090, this is closest multiplier for both 5 and 32
    intermediate_size=14400, # Original Mistral have 14336, this is closest multiplier for both 5 and 32
    num_hidden_layers=5, # Instead of 32 - to make model roughly 1-billion params
    num_attention_heads=32,
    num_key_value_heads=8,
    hidden_act="silu",
    max_position_embeddings=32768,
    initializer_range=0.02,
    rms_norm_eps=1e-5,
    use_cache=True,
    rope_theta=10000.0,
    sliding_window=4096,
    attention_dropout=0.0,
)
model = BitMistralForCausalLM(
    config=config,
    fname_prefix=f"{STORE_DIR}/bitmistal"
).to("cuda:0")

model.add_adapters(
    LoRAAdapter,
    {
        "lora_rank": 128,
    }
)

optimizer = ReLoRAOptimizer(
    model.parameters(),
    model.mergeable_layers(),
    optimizer_cls=AdamW,
    optimizer_params={},
    reset_n_steps=500,
    lr=1e-5,
)
lr_scheduler = LambdaLR(
    optimizer,
    ReLoRASchedulerLambda(
        lr_lambda=lambda step: step / 1000 if step < 1000 else min(1.0 - (step - 50000) / 50000, 1e-5),
        warmup_n_steps=100,
        reset_n_steps=500,
    )
)

optimizer = ReLoRAOptimizer(
    model.parameters(),
    model.mergeable_layers(),
    optimizer_cls=AdamW,
    optimizer_params={},
    reset_n_steps=500,
    lr=1e-5,
)
lr_scheduler = LambdaLR(
    optimizer,
    ReLoRASchedulerLambda(
        lr_lambda=lambda step: step / 1000 if step < 1000 else min(1.0 - (step - 50000) / 50000, 1e-5),
        warmup_n_steps=100,
        reset_n_steps=500,
    )
)

def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, max_length=1024)

# Tokenize all parts of the dataset
tokenized_datasets = dataset_text.map(tokenize_function, batched=True, remove_columns=["text"])
tokenized_datasets

data_collator = DataCollatorForLanguageModeling(tokenizer, 
                                                mlm=False,
                                                pad_to_multiple_of=8)

class GpuMemoryLoggingCallback(TrainerCallback):
    """A custom callback for logging GPU memory usage."""
    
    def on_log(self, args, state, control, logs=None, **kwargs):
        # Check if CUDA is available to avoid errors on CPU-only environments
        if torch.cuda.is_available():
            # Assuming a single-GPU setup here; adjust for multi-GPU as needed
            result = subprocess.run(['nvidia-smi', '--query-gpu=memory.used', '--format=csv,nounits,noheader'],
                                    capture_output=True, text=True)
            memory_usage = result.stdout.strip()
            
            # Convert memory usage to an integer (MB) and log it
            logs['gpu_memory_usage_mb'] = int(memory_usage)
        else:
            logs['gpu_memory_usage_mb'] = 0  # Default to 0 if not using GPU

model.train()
model.gradient_checkpointing_enable()
training_args = TrainingArguments(
    output_dir="./mistral-2b-stored-model",
    per_device_train_batch_size=4,
    gradient_accumulation_steps=4,
    per_device_eval_batch_size=4,
    eval_accumulation_steps=4,
    logging_dir="./mistral-2b-tensorboard-bitwise",
    logging_steps=1,
    save_strategy="steps",
    save_steps=2000,
    evaluation_strategy="steps",
    eval_steps=2000,
    fp16=True,
    gradient_checkpointing=True,
    report_to="tensorboard",
    max_steps=10000,
    # No need to specify data collator here, it's passed to the Trainer constructor
)

# Initialize the Trainer with the data collator
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],  # Assuming these are ready; dynamically tokenized if not
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    optimizers=(optimizer, lr_scheduler),
    callbacks=[GpuMemoryLoggingCallback()],
)

# Train
trainer.train()
```

So far it seems to have far more memory efficiency (9GB consumed in this
method and ~29Gb consumed through a normal training).

The quality seemed comparable, but it was just a start of the training
process, so no conclusions yet.

Before continuing the stuff I will have to debug

    /usr/local/lib/python3.10/dist-packages/torch/serialization.py in _save(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)
        839     pickler = pickle_module.Pickler(data_buf, protocol=pickle_protocol)
        840     pickler.persistent_id = persistent_id
    --> 841     pickler.dump(obj)
        842     data_value = data_buf.getvalue()
        843     zip_file.write_record('data.pkl', data_value, len(data_value))

    AttributeError: Can't pickle local object 'ReLoRASchedulerLambda._wrap_lr_lambda.<locals>._func'
