# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/02_relora.ipynb.

# %% auto 0
__all__ = ['ReLoRAOptimizer', 'is_pickleable', 'ReLoRASchedulerLambda', 'LinearWarmupSchedule']

# %% ../nbs/02_relora.ipynb 4
from typing import Type, Any, Dict, Iterable, Callable, Union, List
import torch
from torch.utils.data import TensorDataset, DataLoader
from .bitlinear import BitLinear
from .adapters import LoRAAdapter, MergeableLayer
from torch.optim import Adam, Optimizer
from torch.optim.lr_scheduler import LambdaLR
import gc
import pickle

# %% ../nbs/02_relora.ipynb 7
class ReLoRAOptimizer(Optimizer):
    def __init__(self,
                 params: Iterable[torch.Tensor] | Iterable[Dict[str, Any]],
                 mergeable_layers: Iterable[MergeableLayer],
                 optimizer_cls: Type[Optimizer],
                 optimizer_params: Dict[str, Any],
                 reset_n_steps: int,
                 lr: float = 1e-3,) -> None:
        params_list = list(params)
        self.inner_params = params_list
        self.optimizer_cls = optimizer_cls
        self.optimizer_params = optimizer_params
        self.lr = lr
        
        # Some trickery around param_groups require me to re-initialize stuff
        self.optimizer = self._initialize_optimizer()
        super(ReLoRAOptimizer, self).__init__(params_list, {})
        self.optimizer = self._initialize_optimizer()
        self._cleanup()

        self.mergeable_layers = mergeable_layers
        self.reset_n_steps = reset_n_steps
        self.made_steps = 0
    
    def _cleanup(self):
        gc.collect()
        torch.cuda.empty_cache()

    def _initialize_optimizer(self) -> Optimizer:
        params = dict(lr=self.lr, **self.optimizer_params)
        return self.optimizer_cls(self.inner_params, **params)
    
    def zero_grad(self, set_to_none: bool = True) -> None:
        return self.optimizer.zero_grad(set_to_none=set_to_none)
    
    def step(self, *args, **kwargs) -> None:
        self.optimizer.step(*args, **kwargs)
        if self.made_steps > 0 and self.made_steps % self.reset_n_steps == 0:
            for layer in self.mergeable_layers:
                layer.merge_adapter()
            self.optimizer.zero_grad(set_to_none=True)
            self.optimizer = None
            self._cleanup()
            self.optimizer = self._initialize_optimizer()
        self.made_steps += 1

    @property
    def param_groups(self):
        groups = []
        for group in self.optimizer.param_groups:
            if 'lr' not in group:
                group['lr'] = self.lr
            groups.append(group)
        return groups
    
    @param_groups.setter
    def param_groups(self, groups):
        self.optimizer.param_groups = groups

    def state_dict(self) -> Dict[str, Any]:
        return dict(self.optimizer.state_dict(), made_steps=self.made_steps)
    
    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
        self.made_steps = state_dict['made_steps']
        return self.optimizer.load_state_dict({
            key: value for key, value in state_dict.items() if key not in {'made_steps'}
        })

# %% ../nbs/02_relora.ipynb 9
def is_pickleable(obj: Any) -> bool:
    try:
        pickle.dumps(obj)
        return True
    except:
        return False

# %% ../nbs/02_relora.ipynb 10
class ReLoRASchedulerLambda:
    def __init__(self, lr_lambda: callable, reset_n_steps: int, warmup_n_steps: int):
        assert is_pickleable(lr_lambda), "lr_lambda should be a pickleable object to use in the training process. " + \
            "Otherwise many popular trainer loop implementations will fail"
        self.lr_lambda = lr_lambda
        self.reset_n_steps = reset_n_steps
        self.warmup_n_steps = warmup_n_steps
    
    def __call__(self, step: int) -> Any:
        if step % self.reset_n_steps < self.warmup_n_steps:
            k = (step % self.reset_n_steps) / self.warmup_n_steps
        else:
            k = 1
        return self.lr_lambda(step) * k

# %% ../nbs/02_relora.ipynb 11
class LinearWarmupSchedule:
    def __init__(self, warmup_steps: int, total_steps: int):
        self.warmup_steps = warmup_steps
        self.total_steps = total_steps
    
    def __call__(self, step: int) -> float:
        if step < self.warmup_steps:
            return step / self.warmup_steps
        return max(0.0, 1.0 - (step - self.warmup_steps) / (self.total_steps - self.warmup_steps))
