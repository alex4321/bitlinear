# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/01_bitlinear.ipynb.

# %% auto 0
__all__ = ['STORAGE_BIT_COUNT', 'STORAGE_DTYPE', 'STORAGE_VALUES_PER_ITEM', 'MAPPING_UINT8_TO_5_PARAMS',
           'quantization_inner_numba', 'build_quantization_group_getter', 'dequantize_weights', 'quantize_weights',
           'BitLinear']

# %% ../nbs/01_bitlinear.ipynb 4
from typing import List, Union, Tuple, Iterable
import torch
import numba as nb
import numpy as np
from .adapters import LinearAdapter, LoRAAdapter, MergeableLayer

# %% ../nbs/01_bitlinear.ipynb 8
STORAGE_BIT_COUNT = 8
STORAGE_DTYPE = torch.ByteTensor

# %% ../nbs/01_bitlinear.ipynb 9
def _get_parameter_count_per_n_bits(n: int) -> int:
    i = 0
    while True:
        j = i + 1
        if 3 ** j > 2 ** n:
            break
        i += 1
    return i


STORAGE_VALUES_PER_ITEM = _get_parameter_count_per_n_bits(STORAGE_BIT_COUNT)
STORAGE_VALUES_PER_ITEM

# %% ../nbs/01_bitlinear.ipynb 12
def _generate_parameter_mappings(parameter_count: int, pad_to_size: int) -> List[List[int]]:
    def _iter(rest_count):
        if rest_count == 0:
            return [[]]
        else:
            result = []
            for p in [-1, 0, 1]:
                for rest in _iter(rest_count-1):
                    result.append([p] + rest)
            return result
    
    response = _iter(parameter_count)
    assert len(response) < pad_to_size
    response += [ [1] * parameter_count ] * (pad_to_size - len(response))
    return response


def _generate_parameter_mappings_tensor(parameter_count: int, pad_to_size: int) -> torch.Tensor:
    return torch.FloatTensor(_generate_parameter_mappings(parameter_count, pad_to_size))

# %% ../nbs/01_bitlinear.ipynb 13
MAPPING_UINT8_TO_5_PARAMS = _generate_parameter_mappings_tensor(
    STORAGE_VALUES_PER_ITEM,
    2 ** STORAGE_BIT_COUNT
)
assert MAPPING_UINT8_TO_5_PARAMS.shape == (256, 5)

# %% ../nbs/01_bitlinear.ipynb 16
def _generate_reverse_mapping_tree(depth):
    counter = 0

    def _iter(inner_depth: int):
        nonlocal counter
        if inner_depth == depth:
            counter_old = counter
            counter += 1
            return counter_old
        else:
            children = []
            for item in [-1, 0, 1]:
                children.append((inner_depth, item, _iter(inner_depth + 1)))
            return children
    
    return _iter(0)


def _render_condition_lines(index, value, inner, eps_str, operator):
    lines = []
    lines.append(f"{operator} -{eps_str} <= (data[{index}] - ({value})) <= {eps_str}:")
    if isinstance(inner, int):
        lines.append(f"  return {inner}")
    elif isinstance(inner, list):
        inner_operator = "if"
        for inner_condition in inner:
            lines += [
                "  " + line
                for line in _render_condition_lines(*inner_condition, eps_str=eps_str, operator=inner_operator)
            ]
            inner_operator = "elif"
    else:
        raise ValueError()
    return lines


def _render_condition(index, value, inner, eps_str, operator):
    return "\n".join(_render_condition_lines(index, value, inner, eps_str, operator))


def _render_multicondition_function(name, conditions):
    parts = []
    operator = "if"
    for condition in conditions:
        parts.append(
            _render_condition(*condition, eps_str="1e-5", operator=operator)
        )
        operator = "elif"
    inner_code = "\n".join(parts)
    inner_code_lines = inner_code.split("\n")
    inner_code_lines = [
        "  " + line for line in inner_code_lines
    ]

    code = f"def {name}(data):\n"
    code += "\n".join(inner_code_lines)
    return code

def build_quantization_group_getter(depth):
    def _build_quantization_inner_numba_row():
        conditions = _generate_reverse_mapping_tree(depth)
        code_func = _render_multicondition_function("quantization_inner_numba_row", conditions)
        exec_locals = {}
        exec(code_func, {}, exec_locals)
        func = exec_locals["quantization_inner_numba_row"]
        return nb.njit(func)
    
    dequantization_inner_numba_row = _build_quantization_inner_numba_row()
    
    @nb.njit(parallel=True)
    def quantization_inner_numba_inner(data_grouped: np.ndarray) -> np.ndarray:
        response = np.zeros((data_grouped.shape[0],), dtype=np.uint8)
        for i in nb.prange(response.shape[0]):
            response[i] = dequantization_inner_numba_row(data_grouped[i])
        return response
    
    def quantization_inner_numba(data: np.ndarray) -> np.ndarray:
        data_grouped = data.reshape((-1, depth))
        response_flatten = quantization_inner_numba_inner(data_grouped)
        response = response_flatten.reshape(list(data.shape[:-1]) + [data.shape[-1] // STORAGE_VALUES_PER_ITEM])
        return response

    return quantization_inner_numba

# %% ../nbs/01_bitlinear.ipynb 17
quantization_inner_numba = build_quantization_group_getter(STORAGE_VALUES_PER_ITEM)
assert (
    quantization_inner_numba(
        MAPPING_UINT8_TO_5_PARAMS[:3 ** STORAGE_VALUES_PER_ITEM].detach().cpu().numpy().astype(np.float32)
    ) == \
    np.arange(3 ** STORAGE_VALUES_PER_ITEM).reshape([-1, 1])
).all()

# %% ../nbs/01_bitlinear.ipynb 19
@torch.no_grad
def dequantize_weights(weight_mapping: torch.Tensor, packed_weights: torch.Tensor, scale: Union[torch.Tensor, float]) \
    -> torch.Tensor:
    weights_per_item = weight_mapping.shape[-1]
    weights_packed_shape = list(packed_weights.shape[:-1]) + \
        [weights_per_item * packed_weights.shape[-1]]
    dequantized_weights_k = weight_mapping[packed_weights.long(), :].view(weights_packed_shape)
    return dequantized_weights_k * scale

# %% ../nbs/01_bitlinear.ipynb 21
@torch.no_grad
def quantize_weights(weights: torch.FloatTensor, mean: Union[torch.Tensor, float]) \
    -> torch.Tensor:
    weights_centered = weights - mean
    weights_sign = weights_centered.sign()
    weights_sign_np = weights_sign.detach().cpu().numpy()
    weigths_group_chosen_np = quantization_inner_numba(weights_sign_np)
    weigths_group_chosen = torch.ByteTensor(weigths_group_chosen_np).to(weights.device)
    return weigths_group_chosen

# %% ../nbs/01_bitlinear.ipynb 24
class BitLinear(MergeableLayer):
    def __init__(self,
                 in_features: int,
                 out_features: int,
                 bias: bool = True,
                 device=None,
                 dtype=None,
                 original_weights_filename: Union[str, None] = None,
                 adapter: Union[None, LinearAdapter]=None):
        super(BitLinear, self).__init__(adapter=adapter)
        assert in_features % STORAGE_VALUES_PER_ITEM == 0

        self.in_features = in_features
        self.out_features = out_features

        if device:
            mapping = MAPPING_UINT8_TO_5_PARAMS.to(device=device)
        else:
            mapping = MAPPING_UINT8_TO_5_PARAMS * 1
        self.mapping, = self._wrap_parameters([mapping])

        self.original_weights_filename = original_weights_filename
        
        initial_linear = torch.nn.Linear(in_features, out_features, bias=bias, device="cpu", dtype=dtype)
        self.mean, self.scale, self.quant_weight = self._wrap_parameters(self._quantize_weight(
            initial_linear.weight,
            device=device
        ))
        
        if bias:
            bias_tensor = initial_linear.bias.data
            if device is not None:
                bias_tensor = bias_tensor.to(device)
            self.bias = torch.nn.Parameter(bias_tensor)
        else:
            self.register_parameter("bias", None)
        if original_weights_filename:
            torch.save(
                initial_linear.weight,
                self.original_weights_filename,
            )        
        
        self.adapter = adapter
    
    def _wrap_parameters(self, tensors: Iterable[torch.Tensor]) -> List[torch.Tensor]:
        return [
            torch.nn.Parameter(tensor, requires_grad=False)
            for tensor in tensors
        ]
    
    def get_dequantized_weights(self) -> torch.Tensor:
        return dequantize_weights(self.mapping, self.quant_weight, self.scale)
    
    def get_stored_weights(self) -> torch.Tensor:
        assert self.original_weights_filename is not None
        weight = torch.load(
            self.original_weights_filename,
            map_location="cpu"
        )
        return weight

    @torch.no_grad
    def _quantize_weight(self, weight: torch.Tensor, device: str) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        weight = weight.cpu()
        mean = weight.mean().cpu()
        scale = weight.abs().mean().cpu()
        qweight = quantize_weights(weight, mean).to(device)
        return mean.to(device), scale.to(device), qweight.to(device)

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        W = self.get_dequantized_weights()
        response = torch.nn.functional.linear(input, W, self.bias)
        if self.adapter:
            adapter = self.adapter(input)
            response = response + adapter
        return response
    
    def _update_parameter(self, parameters: Iterable[torch.nn.Parameter], tensors: Iterable[torch.Tensor]) -> None:
        for parameter, tensor in zip(parameters, tensors):
            parameter.data = tensor
    
    def update_weights(self, weight: torch.Tensor) -> None:
        self._update_parameter(
            [self.mean, self.scale, self.quant_weight],
            self._quantize_weight(weight, self.quant_weight.device)
        )
        torch.save(
            weight,
            self.original_weights_filename,
        )
    
    @torch.no_grad
    def merge_adapter(self) -> None:
        assert self.adapter is not None
        self.update_weights(
            self.get_stored_weights() + self.adapter.calculate_weight_update().to("cpu")
        )
        self.adapter.reset()
