# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/00_adapters.ipynb.

# %% auto 0
__all__ = ['LinearAdapter', 'LoRAAdapter', 'MergeableLayer']

# %% ../nbs/00_adapters.ipynb 4
import torch

# %% ../nbs/00_adapters.ipynb 5
class LinearAdapter(torch.nn.Module):
    def reset(self):
        """
        Reset adapter (so it actually will not influence the output)
        """
        raise NotImplementedError()

    def calculate_weight_update(self):
        """
        Calculate $\Delta W$ matrix for the current adapter state
        """
        raise NotImplementedError()

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        """
        Forward pass
        """
        raise NotImplementedError()

# %% ../nbs/00_adapters.ipynb 6
class LoRAAdapter(LinearAdapter):
    def __init__(self,
                 in_features: int,
                 out_features: int,
                 lora_rank: int,
                 device=None):
        super(LoRAAdapter, self).__init__()
        self.lora_a = torch.nn.Parameter(torch.zeros(lora_rank, in_features, device=device))
        self.lora_b = torch.nn.Parameter(torch.zeros(out_features, lora_rank, device=device))
        self.reset()
    
    def reset(self) -> None:
        torch.nn.init.xavier_uniform_(self.lora_a.data)
        torch.nn.init.zeros_(self.lora_b.data)
        self.lora_a.data.requires_grad = True
        self.lora_b.data.requires_grad = True
    
    def calculate_weight_update(self):
        return self.lora_b.matmul(self.lora_a)

    def forward(self, input: torch.Tensor) -> torch.Tensor:
        return torch.nn.functional.linear(
            torch.nn.functional.linear(input, self.lora_a),
            self.lora_b
        )

# %% ../nbs/00_adapters.ipynb 7
class MergeableLayer(torch.nn.Module):
    def __init__(self, adapter: LinearAdapter) -> None:
        super(MergeableLayer, self).__init__()
        self.adapter = adapter
    
    def merge_adapter(self) -> None:
        raise NotImplementedError()
